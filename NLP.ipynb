{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Part I: Build word representation using Skip-Gram model](#1)\n",
    "# 2. [Part II: Use off-the-shelf Word2Vec package to train](#2)\n",
    "# 3. [Part III: CNN/RNN - Sentiment Analysis](#3)\n",
    "# 4. [Part](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<a id = '1'></a>\n",
    "# Part I: Build word representation using Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data as list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def download(filename):\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename,local_filename)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = download('text8.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data into a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Count, Dictionary, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "def build_dataset(words, n_words):\n",
    "    \n",
    "    # count --> [['UNK',?], (word1, count1), ....]\n",
    "    count = [['UNK', -1]] #replace rare words with UNK\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1)) \n",
    "    \n",
    "    # dictionary --> {'word1': index1}\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    # data --> [index1, index2, ...]    \n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # 'UNK'\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    \n",
    "    # count --> [['UNK',count_0], (word1, count1), ....]\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    # reversed_dictionary --> [{index1: word1}]\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5234, 3081, 12]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', 418391], ('the', 1061396), ('of', 593677)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: \n",
      "[3081 3081   12   12    6    6  195  195]\n",
      "Labels: \n",
      "[[  12]\n",
      " [5234]\n",
      " [3081]\n",
      " [   6]\n",
      " [  12]\n",
      " [ 195]\n",
      " [   6]\n",
      " [   2]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch: ')\n",
    "print(batch)\n",
    "print('Labels: ')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format: index-middle, word-middle, index-left, word-left\n",
      "\n",
      "3081 originated -> 12 as\n",
      "3081 originated -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n"
     ]
    }
   ],
   "source": [
    "print('Format: index-middle, word-middle, index-left, word-left\\n')\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', \n",
    "          labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 29,  5, 64, 90, 42, 20,  1, 79, 49, 97, 35, 92, 50, 11, 27])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph - 1, Embedding Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize weight matrix between -1 and 1, size is v_size * embedding sizwe\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(100, 128) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "embed # Note, shape = (batch_size, embedding_size), i.e., word vec for all inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph - 2, From hidden layer to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss = tf.reduce_mean( #mean over batch\n",
    "      tf.nn.nce_loss(weights = nce_weights,\n",
    "                     biases = nce_biases,\n",
    "                     labels = train_labels,\n",
    "                     inputs = embed,\n",
    "                     num_sampled = num_sampled,\n",
    "                     num_classes = vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After optimization, calculate valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset) # find embedding vec for all valid in\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True) # calculate distance w/ all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_steps = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  290.665863037\n",
      "Average loss at step  2000 :  113.576555559\n",
      "Average loss at step  4000 :  52.9484763718\n",
      "Average loss at step  6000 :  33.1720424192\n",
      "Average loss at step  8000 :  23.7593583269\n",
      "Average loss at step  10000 :  17.7926064858\n",
      "Nearest to may: besides, gb, microsoft, carbonaceous, relays, orbital, ep, intelligent,\n",
      "Nearest to or: and, ada, morocco, pursuit, of, a, victoriae, reginae,\n",
      "Nearest to in: and, of, for, by, from, as, on, to,\n",
      "Nearest to american: vs, pseudocode, cheese, twelve, crater, livejournal, mike, rand,\n",
      "Nearest to use: ada, sherlock, reginae, amo, newsgroup, victoriae, fictional, recollection,\n",
      "Nearest to but: and, otherwise, encampment, vs, alien, economic, predicted, yum,\n",
      "Nearest to that: ufo, and, phi, this, defined, mosque, glamorous, gland,\n",
      "Nearest to the: a, gland, his, vs, victoriae, one, coke, analogue,\n",
      "Nearest to about: file, densities, phi, bckgr, automobile, analogue, psi, modal,\n",
      "Nearest to had: and, senado, vs, lateral, is, interpretations, szil, selective,\n",
      "Nearest to will: ada, reginae, mctaggart, farmers, does, excepting, vs, pounds,\n",
      "Nearest to at: in, and, on, sponsored, digit, UNK, bissau, foote,\n",
      "Nearest to system: victoriae, aim, alma, agave, pyrenees, gb, an, routledge,\n",
      "Nearest to all: boat, coke, vs, mcclellan, purposes, execution, ads, rappers,\n",
      "Nearest to is: was, are, and, has, reginae, rotate, gland, ada,\n",
      "Nearest to it: this, he, gland, a, and, uncle, UNK, assessment,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "\n",
    "    init.run()\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0 and step > 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<a id = '2'></a>\n",
    "# Part II: Use off-the-shelf Word2Vec package to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rouseguy/DeepLearning-NLP/blob/master/notebooks/2.%20word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caesar_file = './data/juliuscaesar.txt'\n",
    "stopword_file  = './data/long_stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_wordsstop_wo  = []\n",
    "with open(stopword_file,'r') as inpFile:\n",
    "    lines = inpFile.readlines()\n",
    "    stop_words_temp = map(lambda x : re.sub('\\n','',x),lines)\n",
    "    stop_words = list(map(lambda x:  re.sub('[^A-Za-z0-9]+', '',x), stop_words_temp))\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    word = word.strip()\n",
    "    word = word.lower()\n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "    if word not in stop_words:\n",
    "        return word\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: they'll --> Cleaned: \n"
     ]
    }
   ],
   "source": [
    "print('Raw: ' + r\"they'll\" + ' --> Cleaned: ' + clean(\"they'll\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: King's --> Cleaned: kings\n"
     ]
    }
   ],
   "source": [
    "print('Raw: ' + r\"King's\" + ' --> Cleaned: ' + clean(\"King's\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentence from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_count = 0\n",
    "sentences = []\n",
    "\n",
    "            \n",
    "with open(caesar_file,'r') as inpFile:\n",
    "    x = inpFile.readlines()\n",
    "    for line in x:\n",
    "        if line is not None or line != '\\n':\n",
    "            words = line.split()\n",
    "            words = map(lambda x: clean(x), words)\n",
    "            words = list(filter(lambda x:True if len(x) > 0 else False, words))\n",
    "            sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['second', 'commoner', 'sir', 'wear', 'shoes'],\n",
       " [],\n",
       " ['work', 'sir', 'holiday']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[107:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = Word2Vec (sentences, \n",
    "                 window=5, \n",
    "                 size=500, \n",
    "                 workers=4, \n",
    "                 min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector length is: 500\n",
      "The vector for word \"second\" is : \n",
      "[ 0.00054155 -0.00027198 -0.00019997  0.00070842 -0.00021237]\n"
     ]
    }
   ],
   "source": [
    "# Example output:\n",
    "print('The vector length is: ' + str(len(model.wv['second'])))\n",
    "print('The vector for word \"second\" is : ')\n",
    "print(model.wv['second'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brutus', 0.43952152132987976),\n",
       " ('caesar', 0.3901950418949127),\n",
       " ('citizen', 0.38464778661727905),\n",
       " ('time', 0.36275291442871094),\n",
       " ('antony', 0.3611658215522766),\n",
       " ('thee', 0.3459341526031494),\n",
       " ('cassius', 0.3410149812698364),\n",
       " ('electronic', 0.3395448327064514),\n",
       " ('messala', 0.33680325746536255),\n",
       " ('good', 0.3312881886959076)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['rome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<a id = '3'></a>\n",
    "# Part III: CNN/RNN - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[https://github.com/rouseguy/DeepLearning-NLP/blob/master/notebooks/3.%20CNN%20-%20Text.ipynb](https://github.com/rouseguy/DeepLearning-NLP/blob/master/notebooks/3.%20CNN%20-%20Text.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from script import data_helpers\n",
    "from script import w2v \n",
    "from script.w2v import train_word2vec\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, SpatialDropout1D, Convolution1D, MaxPooling1D, LSTM\n",
    "from sklearn.cross_validation import train_test_split\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model to get the embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading finished...\n",
      "There is a total of 18779 words in vocabulary\n",
      "The shape of X is: (10662, 56)\n",
      "The shape of Y is: (10662, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data() # Note, x is padded with zero in the end\n",
    "print(\"Loading finished...\")\n",
    "print('There is a total of ' + str(len(vocabulary)) + ' words in vocabulary')\n",
    "print('The shape of X is: ' + str(x.shape)) # 10662 sequences, every sequence has 56 words\n",
    "print('The shape of Y is: ' + str(y.shape)) # 10662 results with either [1,0] - positive or [0,1] - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sequence_length = 56\n",
    "embedding_dim = 20          \n",
    "num_filters = 150\n",
    "filter_size = 3\n",
    "dropout_prob = 0.25\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "\n",
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count                        \n",
    "context = 10        # Context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Word2Vec model '20features_1minwords_10context'\n"
     ]
    }
   ],
   "source": [
    "# train_word2vec\n",
    "embedding_weights = train_word2vec(x, vocabulary_inv, embedding_dim, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18779, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.11376537, -0.13623959, -0.17433217, ...,  0.34611851,\n",
       "         -0.19512145, -0.14178257],\n",
       "        [ 0.0526082 , -0.07634247, -0.20783381, ...,  0.39987352,\n",
       "          0.00308891, -0.22333454],\n",
       "        [-0.01896649, -0.23291215, -0.18632506, ...,  0.21834175,\n",
       "          0.04105491, -0.16411212],\n",
       "        ..., \n",
       "        [ 0.02709346, -0.28432941, -0.29434878, ...,  0.40582779,\n",
       "          0.07189913, -0.19080783],\n",
       "        [ 0.12260104, -0.37218949, -0.11956801, ...,  0.16573152,\n",
       "          0.01496829, -0.33405513],\n",
       "        [ 0.02421027, -0.18543144, -0.29883066, ...,  0.07891279,\n",
       "          0.08901211, -0.1006508 ]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.append(x,y,axis = 1)\n",
    "train, test = train_test_split(data, test_size = 0.15,random_state = 0)\n",
    "X_test = test[:,:-2]\n",
    "Y_test = test[:,-2:]\n",
    "X_train = train[:,:-2]\n",
    "Y_train = train[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9062, 56)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training - 1, using pretrained embedding, and 1-d CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 56, 20)            375580    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 54, 150)           9150      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 27, 150)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 27, 150)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4050)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 8102      \n",
      "=================================================================\n",
      "Total params: 392,832\n",
      "Trainable params: 392,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(vocabulary), \n",
    "                    output_dim = embedding_dim, \n",
    "                    input_length = sequence_length,\n",
    "                    weights = embedding_weights))\n",
    "\n",
    "model.add(Convolution1D(filters = num_filters,\n",
    "                         kernel_size = filter_size,\n",
    "                         strides = 1,\n",
    "                         padding = 'valid',\n",
    "                         activation = 'relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(dropout_prob))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/1200/1*h_L7fSoQhipTHFULgXmHyQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9062 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "9062/9062 [==============================] - 3s 297us/step - loss: 0.6626 - acc: 0.5979 - val_loss: 0.6803 - val_acc: 0.6144\n",
      "Epoch 2/5\n",
      "9062/9062 [==============================] - 2s 219us/step - loss: 0.5595 - acc: 0.7143 - val_loss: 0.5986 - val_acc: 0.7131\n",
      "Epoch 3/5\n",
      "9062/9062 [==============================] - 2s 218us/step - loss: 0.4709 - acc: 0.7796 - val_loss: 0.5416 - val_acc: 0.7231\n",
      "Epoch 4/5\n",
      "9062/9062 [==============================] - 2s 222us/step - loss: 0.4020 - acc: 0.8169 - val_loss: 0.5326 - val_acc: 0.7669\n",
      "Epoch 5/5\n",
      "9062/9062 [==============================] - 2s 219us/step - loss: 0.3475 - acc: 0.8456 - val_loss: 0.5388 - val_acc: 0.7444\n",
      "Test score: 0.538783634305\n",
      "Test accuracy: 0.744375\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=5)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "serry wants to blend politics and drama , an admirable ambition it 's too bad that the helping hand he uses to stir his ingredients is also a heavy one --> 0 --> 0\n",
      "\n",
      "an amateurish , quasi improvised acting exercise shot on ugly digital video --> 0 --> 0\n",
      "\n",
      "playing a role of almost bergmanesque intensity bisset is both convincing and radiant --> 1 --> 1\n",
      "\n",
      "no big whoop , nothing new to see , zero thrills , too many flashbacks and a choppy ending make for a bad film --> 0 --> 0\n",
      "\n",
      "please , someone , stop eric schaeffer before he makes another film --> 0 --> 0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    sentence = '\\n'\n",
    "    for num in X_test[_]:\n",
    "        word = vocabulary_inv[num]\n",
    "        if word != '<PAD/>':\n",
    "            sentence = sentence + word + ' '\n",
    "    pred = preds[_]\n",
    "    label = 0 if Y_test[_][0] == 1 else 1\n",
    "    print(sentence + '--> ' + str(pred) + ' --> ' + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training - 2, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 56, 20)            375580    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_15 (Spatia (None, 56, 20)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 120)               67680     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 242       \n",
      "=================================================================\n",
      "Total params: 443,502\n",
      "Trainable params: 443,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(vocabulary), \n",
    "                    output_dim = embedding_dim, \n",
    "                    input_length = sequence_length,               \n",
    "                    mask_zero = True, #https://keras.io/layers/embeddings/\n",
    "                    weights = embedding_weights))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(LSTM(units = 120, \n",
    "               dropout = 0.2,  \n",
    "               recurrent_dropout = 0.2,\n",
    "               return_sequences = False)) # (Batch size, time steps, units) - with return_sequences=True\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://d3ansictanv2wj.cloudfront.net/SentimentAnalysis16-38b6f3cbb7bae622fe0ba114db188666.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9062 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "9062/9062 [==============================] - 19s 2ms/step - loss: 0.6923 - acc: 0.5216 - val_loss: 0.6823 - val_acc: 0.6056\n",
      "Epoch 2/5\n",
      "9062/9062 [==============================] - 17s 2ms/step - loss: 0.6639 - acc: 0.5964 - val_loss: 0.6290 - val_acc: 0.6669\n",
      "Epoch 3/5\n",
      "9062/9062 [==============================] - 16s 2ms/step - loss: 0.5968 - acc: 0.6768 - val_loss: 0.5703 - val_acc: 0.7131\n",
      "Epoch 4/5\n",
      "9062/9062 [==============================] - 17s 2ms/step - loss: 0.5132 - acc: 0.7510 - val_loss: 0.5273 - val_acc: 0.7425\n",
      "Epoch 5/5\n",
      "9062/9062 [==============================] - 18s 2ms/step - loss: 0.4474 - acc: 0.7907 - val_loss: 0.5068 - val_acc: 0.7544\n",
      "Test score: 0.506822817326\n",
      "Test accuracy: 0.754375\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=5)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
