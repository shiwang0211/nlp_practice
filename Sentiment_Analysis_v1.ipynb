{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/adeshpande3/LSTM-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.; Source: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', '0.60308', '-0.32024', '0.088857', '-0.55176', '0.53182', '0.047069', '-0.36246', '0.0057018', '-0.37665', '0.22534', '-0.13534', '0.35988', '-0.42518', '0.071324', '0.77065', '0.56712', '0.41226', '0.12451', '0.1423', '-0.96535', '-0.39053', '0.34199', '0.56969', '0.031635', '0.69465', '-1.9216', '-0.67118', '0.57971', '0.86088', '-0.59105', '3.7787', '0.30431', '-0.043103', '-0.42398', '-0.063915', '-0.066822', '0.061983', '0.56332', '-0.22335', '-0.47386', '-0.47021', '0.091714', '0.14778', '0.63805', '-0.14356', '-0.0022928', '-0.315', '-0.25187', '-0.26879', '0.36657']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 400000\n",
    "embed_size = 50\n",
    "\n",
    "wordVectors = np.random.normal(0, size=[vocab_size, embed_size])\n",
    "wordVectors = wordVectors.astype(np.float32) ## to be consistent\n",
    "wordsList = []\n",
    "\n",
    "with open('glove.6B.50d.txt', encoding=\"utf-8\", mode=\"r\") as textFile:\n",
    "    word_id = 0\n",
    "    for line in textFile:\n",
    "        line = line.split()\n",
    "        if word_id == 100:\n",
    "            print(line)\n",
    "        word = line[0]\n",
    "        wordsList.append(word)\n",
    "        wordVectors[word_id] = np.array(line[1:], dtype=np.float32)\n",
    "        word_id += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word Vector:  (400000, 50)\n",
      "Embedding vector of first word:  [ 0.41800001  0.24968    -0.41242     0.1217      0.34527001] ...\n",
      "The index of word `good` is:  219\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Word Vector: ', wordVectors.shape)\n",
    "print('Embedding vector of first word: ',wordVectors[0][:5], '...')\n",
    "print('The index of word `good` is: ', wordsList.index('good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `WordsList` is a list of words (40,000)\n",
    "- `wordVectors` is the embedding vectors for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of sentence coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adeshpande3/LSTM-Sentiment-Analysis/raw/4bb7b1e8c0e8e9f7f649d1f68cb34db0b2b6675e/Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   41   804     0  1005    15  7446     5 13767     0     0]\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = embed_size #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate length of comments to determine sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['data/positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['data/negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assign Sequence Length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of translating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before cleaning, raw text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[0] \n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(cleanSentences(lines))\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([174943,    152,     14,      7,   7362,   2841,     20,   1421,\n",
       "           22,      0,    215,     79,     19,     77,     68,   1009,\n",
       "           59,    164,    214,    125,     19,   2562,    192,   1678,\n",
       "           82,      6,      0,   3174,   8104,    410,    285,      4,\n",
       "          733,     12, 174943,   7984,  15303,     14,    181,   2386,\n",
       "            4,   2532,     73,     14,   2562,      0,  14170,      4,\n",
       "         3981,   7980,      0,  34401,    543,     38,     86,    253,\n",
       "          248,    131,     44,  22495,   2562,  31166,      0,  91887,\n",
       "            3,      0,   1115,    794,     64,   9794,    285,      3,\n",
       "            0,    888,     41,   1522,      5,     44,    543,     61,\n",
       "           41,    822,      0,   1942,      6,     42,      7,   1283,\n",
       "         2648,    977,      4,   6292,    135,      0,    164,     41,\n",
       "         1040,   3151,     22,    152,      7,   2392,    331,   5537,\n",
       "        14663,    187,      4,  11739,     48,      3,    392,   2562,\n",
       "         1283,   3143,      4, 174943,    152,     41,   1543,     12,\n",
       "          109,   3574,      3,    192,    464,    269,     12, 174943,\n",
       "          152,     14,    372,  19386,    102,      7,  16214,     12,\n",
       "           20, 228955,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter < maxSeqLength:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 250)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('./script/idsMatrix.npy')\n",
    "ids.shape #25000 sentence with 250 words each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get batch of train samples with half positive and half negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x1,  250\n",
      "Length of x2,  250\n",
      "...Batch size\n",
      "Y,  [[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "batchSize = 10\n",
    "arr_labels = getTrainBatch()\n",
    "print('Length of x1, ', len(arr_labels[0][0])) # Shape of arr is [batch_size, max_sequence]\n",
    "print('Length of x2, ', len(arr_labels[0][1])) # Shape of arr is [batch_size, max_sequence]\n",
    "print('...Batch size')\n",
    "print('Y, ', arr_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adeshpande3/LSTM-Sentiment-Analysis/raw/4bb7b1e8c0e8e9f7f649d1f68cb34db0b2b6675e/Images/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "n_classes = 2\n",
    "iterations = 10000  # 100000\n",
    "learning_rate = 0.001\n",
    "numDimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32,[batchSize, maxSeqLength]) # Note it is consistent with `arr` from next_batch function\n",
    "y = tf.placeholder(tf.int32,[batchSize, n_classes]) # Note it is consistent with `label` from next_batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32) # the vector after embedding\n",
    "data = tf.nn.embedding_lookup(wordVectors, x) # pay attention to the shape of `x` and `data`\n",
    "data = tf.unstack(data, maxSeqLength, 1) # https://www.tensorflow.org/api_docs/python/tf/unstack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250 , Element: Tensor(\"unstack_1:0\", shape=(24, 50), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Length:', len(data), ', Element:', data[0])\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adeshpande3/LSTM-Sentiment-Analysis/raw/c25c41adaa68a0968bdc3540a71b0791b76860cd/Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a typical single layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = rnn.DropoutWrapper(cell = lstmCell, output_keep_prob = 0.75)\n",
    "outputs, _ = tf.nn.static_rnn(lstmCell, data, dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_weights = tf.Variable(tf.random_normal([lstmUnits, n_classes]))\n",
    "out_bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "prediction = tf.matmul(outputs[-1], out_weights)+ out_bias\n",
    "loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction, labels = y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(opt, {x: nextBatch, y: nextBatchLabels})\n",
    "   \n",
    "    #Calculate training error \n",
    "    if (i % 10 == 0):\n",
    "        acc=sess.run(accuracy,feed_dict={x:nextBatch, y:nextBatchLabels})\n",
    "        los=sess.run(loss,feed_dict={x:nextBatch, y:nextBatchLabels})\n",
    "        print('For iter ',i,', Accuracy: ', acc, ' ,Loss: ',los)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 91.6666686535\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 91.6666686535\n",
      "Accuracy for this batch: 83.3333313465\n",
      "Accuracy for this batch: 95.8333313465\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 87.5\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {x: nextBatch, y: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train another CNN model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(24, 250, 50, 1) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cnn = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32) # Same as LTSM\n",
    "data_cnn = tf.nn.embedding_lookup(wordVectors, x)  # Same as LTSM\n",
    "data_cnn = tf.reshape(data_cnn, [batchSize,maxSeqLength,numDimensions,1]) # Reshape to 3D, first 1 + 3d\n",
    "data_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 2 # Number of words per stride\n",
    "num_filters = 4 # Number of filters, matching the figures\n",
    "filter_shape = [filter_size, embed_size, 1, num_filters] # `1` is number of channels\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(data_cnn, W_conv1, \n",
    "                                  strides=[1, 1, 1, 1], padding='VALID') + b_conv1) # Note, cannot use SAME\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, maxSeqLength - filter_size + 1, 1, 1], \n",
    "                         strides=[1, 1, 1, 1], padding='VALID')\n",
    "h_pool1_flat = tf.reshape(h_pool1, [-1, num_filters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(24, 249, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_2:0' shape=(24, 1, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(24, 4) dtype=float32>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_ = tf.placeholder(tf.float32)\n",
    "h_pool1_flat_drop= tf.nn.dropout(h_pool1_flat, keep_prob_)\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([num_filters, n_classes], stddev= 0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape = [n_classes]))\n",
    "prediction_ = tf.nn.relu(tf.matmul(h_pool1_flat_drop, W_fc1) + b_fc1) # use relu as activition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(24, 4) dtype=float32>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1_flat_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(24, 2) dtype=float32>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction_, labels = y))\n",
    "opt_ = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_)\n",
    "correctPred_ = tf.equal(tf.argmax(prediction_, 1), tf.argmax(y,1))\n",
    "accuracy_ = tf.reduce_mean(tf.cast(correctPred_, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(opt_, {x: nextBatch, y: nextBatchLabels, keep_prob_: 0.50})\n",
    "\n",
    "    if (i % 10 == 0):\n",
    "        acc_=sess.run(accuracy_,feed_dict={x:nextBatch, y:nextBatchLabels, keep_prob_: 1.00})\n",
    "        los_=sess.run(loss_,feed_dict={x:nextBatch, y:nextBatchLabels, keep_prob_: 1.00})\n",
    "        print('For iter ',i,', Accuracy: ', acc_, ' ,Loss: ',los_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do: Combine CNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(24, 249, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_3:0' shape=(24, 124, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 1, 1], \n",
    "                         strides=[1, 2, 1, 1], padding='VALID')\n",
    "h_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_2:0' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_2:1' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_2:2' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_2:3' shape=(24, 124, 1) dtype=float32>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#goal: Length: 250 , Element: Tensor(\"unstack:0\", shape=(24, 50), dtype=float32)\n",
    "h_unstack = tf.unstack(h_pool2, axis = 3)\n",
    "h_unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(24, 124, 4) dtype=float32>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_concat = tf.concat(h_unstack, axis = 2)\n",
    "h_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 124 , Element: Tensor(\"unstack_3:0\", shape=(24, 4), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ = tf.unstack(h_concat, 124 , 1) # https://www.tensorflow.org/api_docs/python/tf/unstack \n",
    "print('Length:', len(data_), ', Element:', data_[0])\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then follows the typical LSTM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
