{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/adeshpande3/LSTM-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 400000\n",
    "embed_size = 50\n",
    "\n",
    "wordVectors = np.random.normal(0, size=[vocab_size, embed_size])\n",
    "wordVectors = wordVectors.astype(np.float32) ## to be consistent\n",
    "wordsList = []\n",
    "\n",
    "with open('glove.6B.50d.txt', encoding=\"utf-8\", mode=\"r\") as textFile:\n",
    "    word_id = 0\n",
    "    for line in textFile:\n",
    "        line = line.split()\n",
    "        #format of line: word, v1, v2, ..., v5\n",
    "        word = line[0]\n",
    "        wordsList.append(word)\n",
    "        wordVectors[word_id] = np.array(line[1:], dtype=np.float32)\n",
    "        word_id += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word Vector:  (400000, 50)\n",
      "Embedding vector of first word:  [ 0.41800001  0.24968    -0.41242     0.1217      0.34527001] ...\n",
      "The index of word `good` is:  219\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Word Vector: ', wordVectors.shape)\n",
    "print('Embedding vector of first word: ',wordVectors[0][:5], '...')\n",
    "print('The index of word `good` is: ', wordsList.index('good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of sentence coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adeshpande3/LSTM-Sentiment-Analysis/raw/4bb7b1e8c0e8e9f7f649d1f68cb34db0b2b6675e/Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   41   804     0  1005    15  7446     5 13767     0     0]\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = embed_size #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate length of comments to determine sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1fefcba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assign Sequence Length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of translating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before cleaning, raw text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead).\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a realistic view of homelessness unlike say how citizen kane gave a realistic view of lounge singers or titanic gave a realistic view of italians you idiots many of the jokes fall flat but still this film is very lovable in a way many comedies are not and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive its not the fisher king but its not crap either my only complaint is that brooks should have cast someone else in the lead i love mel as a director and writer not so much as a lead\n"
     ]
    }
   ],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(cleanSentences(lines))\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   37,    14,  2407,     0,    96, 37314,   319,  7158,     0,\n",
       "        6469,  8828,  1085,    47,  9703,    20,   260,    36,   455,\n",
       "           7,  7284,  1139,     3, 26494,  2633,   203,   197,  3941,\n",
       "       12739,   646,     7,  7284,  1139,     3, 11990,  7792,    46,\n",
       "       12608,   646,     7,  7284,  1139,     3,  8593,    81, 36381,\n",
       "         109,     3,     0,  8735,   807,  2983,    34,   149,    37,\n",
       "         319,    14,   191, 31906,     6,     7,   179,   109, 15402,\n",
       "          32,    36,     5,     4,  2933,    12,   138,     6,     7,\n",
       "         523,    59,    77,     3,     0,    96,  4246, 30006,   235,\n",
       "           3,   908,    14,  4702,  4571,    47,    36,     0,  6429,\n",
       "         691,    34,    47,    36, 35404,   900,   192,    91,  4499,\n",
       "          14,    12,  6469,   189,    33,  1784,  1318,  1726,     6,\n",
       "           0,   410,    41,   835, 10464,    19,     7,   369,     5,\n",
       "        1541,    36,   100,   181,    19,     7,   410,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter < maxSeqLength:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous results (skipped some processing codes here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    37,     14,   2407, 201534,     96,  37314,    319,   7158,\n",
       "       201534,   6469,   8828,   1085,     47,   9703,     20,    260,\n",
       "           36,    455,      7,   7284,   1139,      3,  26494,   2633,\n",
       "          203,    197,   3941,  12739,    646,      7,   7284,   1139,\n",
       "            3,  11990,   7792,     46,  12608,    646,      7,   7284,\n",
       "         1139,      3,   8593,     81,  36381,    109,      3, 201534,\n",
       "         8735,    807,   2983,     34,    149,     37,    319,     14,\n",
       "          191,  31906,      6,      7,    179,    109,  15402,     32,\n",
       "           36,      5,      4,   2933,     12,    138,      6,      7,\n",
       "          523,     59,     77,      3, 201534,     96,   4246,  30006,\n",
       "          235,      3,    908,     14,   4702,   4571,     47,     36,\n",
       "       201534,   6429,    691,     34,     47,     36,  35404,    900,\n",
       "          192,     91,   4499,     14,     12,   6469,    189,     33,\n",
       "         1784,   1318,   1726,      6, 201534,    410,     41,    835,\n",
       "        10464,     19,      7,    369,      5,   1541,     36,    100,\n",
       "          181,     19,      7,    410,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('idsMatrix.npy')\n",
    "ids[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get batch of train samples with half positive and half negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x1,  250\n",
      "Length of x2,  250\n",
      "...Batch size\n",
      "Y,  [[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "batchSize = 10\n",
    "arr_labels = getTrainBatch()\n",
    "print('Length of x1, ', len(arr_labels[0][0])) # Shape of arr is [batch_size, max_sequence]\n",
    "print('Length of x2, ', len(arr_labels[0][1])) # Shape of arr is [batch_size, max_sequence]\n",
    "print('...Batch size')\n",
    "print('Y, ', arr_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adeshpande3/LSTM-Sentiment-Analysis/raw/4bb7b1e8c0e8e9f7f649d1f68cb34db0b2b6675e/Images/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "n_classes = 2\n",
    "iterations = 100  # 100000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32,[batchSize, maxSeqLength]) # Note it is consistent with `arr` from next_batch function\n",
    "y = tf.placeholder(tf.int32,[batchSize, n_classes]) # Note it is consistent with `label` from next_batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32) # the vector after embedding\n",
    "data = tf.nn.embedding_lookup(wordVectors, x) # pay attention to the shape of `x` and `data`\n",
    "data = tf.unstack(data, maxSeqLength, 1) # https://www.tensorflow.org/api_docs/python/tf/unstack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 250 , Element: Tensor(\"unstack:0\", shape=(24, 50), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Length:', len(data), ', Element:', data[0])\n",
    "print() \n",
    "# maxSeqLength * [batch_size, numDimensions]\n",
    "# time_steps * [batch_size, n_input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a typical single layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmCell = rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = rnn.DropoutWrapper(cell = lstmCell, output_keep_prob = 0.75)\n",
    "outputs, _ = tf.nn.static_rnn(lstmCell, data, dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_weights = tf.Variable(tf.random_normal([lstmUnits, n_classes]))\n",
    "out_bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "prediction = tf.matmul(outputs[-1], out_weights)+ out_bias\n",
    "loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction, labels = y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# tf.summary.scalar('Loss', loss)\n",
    "# tf.summary.scalar('Accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "# logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  0 , Accuracy:  0.416667  ,Loss:  1.00945\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(opt, {x: nextBatch, y: nextBatchLabels})\n",
    "   \n",
    "    #Calculate training error \n",
    "    if (i % 10 == 0):\n",
    "        acc=sess.run(accuracy,feed_dict={x:nextBatch, y:nextBatchLabels})\n",
    "        los=sess.run(loss,feed_dict={x:nextBatch, y:nextBatchLabels})\n",
    "        print('For iter ',i,', Accuracy: ', acc, ' ,Loss: ',los)\n",
    "\n",
    "#     #Write summary to Tensorboard\n",
    "#     if (i % 50 == 0):\n",
    "#         summary = sess.run(merged, {x: nextBatch, y: nextBatchLabels})\n",
    "#         writer.add_summary(summary, i)\n",
    "        \n",
    "#     #Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 66.6666686535\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {x: nextBatch, y: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Define CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(24, 250, 50, 1) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cnn = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32) # Same as LTSM\n",
    "data_cnn = tf.nn.embedding_lookup(wordVectors, x)  # Same as LTSM\n",
    "data_cnn = tf.reshape(data_cnn, [batchSize,maxSeqLength,numDimensions,1]) # Reshape to 3D, first 1 + 3d\n",
    "data_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_size = 2 # Number of words per stride\n",
    "num_filters = 4 # Number of filters, matching the figures\n",
    "filter_shape = [filter_size, embed_size, 1, num_filters] # `1` is number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(data_cnn, W_conv1, \n",
    "                                  strides=[1, 1, 1, 1], padding='VALID') + b_conv1) # Note, cannot use SAME\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, maxSeqLength - filter_size + 1, 1, 1], \n",
    "                         strides=[1, 1, 1, 1], padding='VALID')\n",
    "h_pool1_flat = tf.reshape(h_pool1, [-1, num_filters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(24, 249, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_3:0' shape=(24, 1, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(24, 4) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_ = tf.placeholder(tf.float32)\n",
    "h_pool1_flat_drop= tf.nn.dropout(h_pool1_flat, keep_prob_)\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([num_filters, n_classes], stddev= 0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape = [n_classes]))\n",
    "prediction_ = tf.nn.relu(tf.matmul(h_pool1_flat_drop, W_fc1) + b_fc1) # use relu as activition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(24, 4) dtype=float32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1_flat_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_ = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction_, labels = y))\n",
    "opt_ = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_)\n",
    "correctPred_ = tf.equal(tf.argmax(prediction_, 1), tf.argmax(y,1))\n",
    "accuracy_ = tf.reduce_mean(tf.cast(correctPred_, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  0 , Accuracy:  0.541667  ,Loss:  0.681307\n",
      "For iter  10 , Accuracy:  0.625  ,Loss:  0.684197\n",
      "For iter  20 , Accuracy:  0.541667  ,Loss:  0.690056\n",
      "For iter  30 , Accuracy:  0.625  ,Loss:  0.690871\n",
      "For iter  40 , Accuracy:  0.5  ,Loss:  0.697458\n",
      "For iter  50 , Accuracy:  0.541667  ,Loss:  0.695283\n",
      "For iter  60 , Accuracy:  0.5  ,Loss:  0.689254\n",
      "For iter  70 , Accuracy:  0.5  ,Loss:  0.687342\n",
      "For iter  80 , Accuracy:  0.458333  ,Loss:  0.697901\n",
      "For iter  90 , Accuracy:  0.5  ,Loss:  0.695119\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(opt_, {x: nextBatch, y: nextBatchLabels, keep_prob_: 0.50})\n",
    "\n",
    "    if (i % 10 == 0):\n",
    "        acc_=sess.run(accuracy_,feed_dict={x:nextBatch, y:nextBatchLabels, keep_prob_: 1.00})\n",
    "        los_=sess.run(loss_,feed_dict={x:nextBatch, y:nextBatchLabels, keep_prob_: 1.00})\n",
    "        print('For iter ',i,', Accuracy: ', acc_, ' ,Loss: ',los_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine CNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(24, 249, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_7:0' shape=(24, 124, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 1, 1], \n",
    "                         strides=[1, 2, 1, 1], padding='VALID')\n",
    "h_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_7:0' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_7:1' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_7:2' shape=(24, 124, 1) dtype=float32>,\n",
       " <tf.Tensor 'unstack_7:3' shape=(24, 124, 1) dtype=float32>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#goal: Length: 250 , Element: Tensor(\"unstack:0\", shape=(24, 50), dtype=float32)\n",
    "h_unstack = tf.unstack(h_pool2, axis = 3)\n",
    "h_unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_18:0' shape=(24, 124, 4) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_concat = tf.concat(h_unstack, axis = 2)\n",
    "h_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 124 , Element: Tensor(\"unstack_11:0\", shape=(24, 4), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ = tf.unstack(h_concat, 124 , 1) # https://www.tensorflow.org/api_docs/python/tf/unstack \n",
    "print('Length:', len(data_), ', Element:', data_[0])\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then follows the typical LSTM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
