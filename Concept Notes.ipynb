{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collaborative filtering\n",
    "text clustering\n",
    "NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLP and Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language Modeling\n",
    "Task Definition: Predict next word\n",
    "\n",
    "### N-gram model\n",
    "   - Prototype: P(L = 3, e1 = I, e2 = am, e3 = WS) = P(e1 = I) \\* P(e2 = am | e1 = I) \\* P(e3 = WS | e1 = I, e2 = am) \\* P(e4 = EoS |e1 = I, e2 = am, e3 = WS) \n",
    "       - NOTE: from beginning of sentence\n",
    "   - Advantage of N-gram: Using count of different length of grams as they shown in corpus\n",
    "   - For example: 2-gram (bigram). Calculate probs of \"I am\", \"am WS\", \"WS EoS\" instead of \"I am WS EoS\"\n",
    "   - Main problem: **Sparsity**, some senetence may not appear in training set, joint probability will be zero (The same problem as Prototype)\n",
    "   - Fix by **smoothing**/**interpolation**: $P(e_t|e_{t-1} = (1-\\alpha)P_{ML}(e_t|e_{t-1}) + \\alpha P_{ML}(e_t)$\n",
    "       - A combination of unigram and bigram to ensure P>0\n",
    "       - Variation: more grams, context-dependent alpha, etc\n",
    "   - Fix unknown words by adding a \"**unk**\" word\n",
    "       - Remove singletopns or words only appearing once in training corpus\n",
    "\n",
    "\n",
    "### NN Model with Fixed Window Size      \n",
    "   - No Sparsity problem (using input embedding M, so possible to traet similar words similarly during prediction)\n",
    "   - Model size reduced (Instead of learning all probs {a, b, c} X {A, B, C}, Neural network learn weights to represent the quadratic combination)\n",
    "   - Ability to skip a previous word\n",
    "   - BUT: X do not share weight, and how to decide window size\n",
    "    \n",
    "    \n",
    "<img src=\"http://kiyukuta.github.io/_images/nnlm_bengio.png\" width=\"500\">\n",
    "\n",
    "### RNN Model\n",
    "   - Any sequence length will work\n",
    "   - Weights shared, model size doesn't increase\n",
    "   - BUT: computation is slow (why) and cannot access information from many steps back\n",
    "   - Others applications of RNN\n",
    "        * One-to-one: tagging each word\n",
    "        * many-to-one: sentiment analysis\n",
    "        * Encoder module: example: element-wise max of all hidden states -> as input for further NN model\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*q1wyldq3Nm5pT266eXdfzA.png\" width=\"400\">\n",
    "\n",
    "\n",
    "### Evaluation of LM\n",
    "- Given 1) Test dataset, and 2) trained language model P with parameter $\\theta$\n",
    "- Log likelihood $log(E_{test};\\theta) = \\sum_{E\\in E_{test}}{log[P(E;\\theta)]}$\n",
    "- Perplexity: $ ppl(E_{test};\\theta) = exp(-log(E_{test};\\theta) / len(E_{test}))$\n",
    "    - ppl = 1: perfect\n",
    "    - ppl = Vocabulary size: random model\n",
    "    - ppl = +inf: worst model\n",
    "    - ppl = some value $v$: need to pick $v$ values on average to get the correct one\n",
    "\n",
    "    \n",
    "### Alternative tasks in NLP - *Conditioned* Language Models\n",
    "   -  Speech recognition\n",
    "   -  Machine translation\n",
    "   -  Generate summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Word Embedding / Word2vec\n",
    "\n",
    "- Why other Options not working\n",
    "    * One-hot vectors (vocabulary list too big; No similarity measurement; how about new words)\n",
    "    * Co-currence vector (matrix given a certain window size, # of times two words are together)->Sparsity\n",
    "    * Singular Vector Decomposition (SVD) for cocurrence matrix (too expensive)\n",
    "    * Use a word's context to represent --> Word embedding\n",
    "    \n",
    "    \n",
    "    \n",
    "- Key Components\n",
    "    * Center word *c*, context word *o*\n",
    "    * Two vectors for each word *w*: $ v_w $ and $ u_w $. $\\theta$ contains all *u* and *v* (Input and Output Vector)\n",
    "    * For example: $ P( w_2|w_1 ) = P(w_2|w_1;  u_{w2}, v_{w1}, \\theta )$\n",
    "    * Loss Function: $ J(\\theta) = -\\frac{1}{T}\\sum_{t}\\sum_{m\\in window} P(w_{t+j}|w_t)$\n",
    "    * Calculate u*v for each word, and use softmax to derive probability\n",
    "      - $ P(O|C) = \\frac{exp(u_o^T v_c)}{\\sum_{w}exp(u_w^T v_c)} $  \n",
    "      - $w$ is entire vocabulary\n",
    "        \n",
    "    * After optimization for loss, get two vectors for each word. Combine or Use *u* or Use *v*\n",
    "    \n",
    "    \n",
    "    \n",
    "- Variation\n",
    "    * Skip-grams (SG):given center, predict context\n",
    "    * Countinous Bag of Words (CBOW):given bag of context, predict center\n",
    "    * Negative sampling (maximize p of actual context + minimize p of random context i.e. noise)\n",
    "    * GloVe: combine count-based and direct-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification\n",
    "## Word Window Classification\n",
    "- Difference with typical ML: learn both **W** and word vectors **x**\n",
    "- Task definition: classify a word in its *Context Window*\n",
    "    * Advantage: Do not train single word: ambiguity\n",
    "    * Advantage: Do not just average over window: lose position information\n",
    "    * Get a vector X with length of 5d where 5 is window size and d is embedding size\n",
    "    * Predict y based on softmax of WX and minimize cross-entropy error\n",
    "    \n",
    "    \n",
    "- Example: NER (*Named Entity Recognition*)\n",
    "    * 'Museums in Paris are good\". Binary task: whether Paris is a *location* or not.\n",
    "    \n",
    "    \n",
    "- What happens for word embedding x:\n",
    "    * Updated just as weigh W\n",
    "    * Pushed into an area helpful for classification task\n",
    "    * Example: $X_{in}$ may be a sign for location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.stack.imgur.com/a6CJc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bicepjai/Deep-Survey-Text-Classification/master/images/paper_02_cnn_sent_model.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "## Problem definition\n",
    "- Neural Machine Translation (NMT)\n",
    "- Sequence-to-Sequence(seq2seq) architecture\n",
    "- Difference from SMT (Statistical MT): calculate P(y|x) directly instead of using Bayes\n",
    "- Advantage: Single NN, less human engineering\n",
    "- Disadvantage: less interpretable, less control\n",
    "- Figure (TBA)\n",
    "\n",
    "## Main Components\n",
    "- Encoder RNN: encode source sentence, generate hidden state\n",
    "- Decoder RNN: **Language Model**, generate target sentence using outputs from encoder RNN; predict next word in *y* conditional on input *x*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1585/1*sO-SP58T4brE9EHazHSeGA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Allow information from future inputs\n",
    "    * LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "\n",
    "- Greedy decoding problem\n",
    "    * Instead of generating argmax each step, use beam search.\n",
    "    * Keep *k* most probable translations\n",
    "    * Exactly *k* nodes at each time step *t*\n",
    "    * *Note*: Length bias, prefer shorter sentence because the log(P) accumulates. Can add prior for sentence length to compensate.\n",
    "    \n",
    "<img src=\"./figure/beam.png\" width=\"300\">\n",
    "https://arxiv.org/pdf/1703.01619.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention model\n",
    "\n",
    "### General Advantage\n",
    "\n",
    "- Focus on certain parts of source (Instead of encoding whole sentence in **one** hidden vector.)\n",
    "- Provides shortcut / Bypass bottleneck\n",
    "- Get some interpretable results and learn alignment\n",
    "- \"Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoderâ€™s LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong attention mechanism\n",
    "\n",
    "\n",
    "\n",
    "1. Get encoder hidden states: $ h_1, ..h_k,..., h_N $\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1})$<br/><br/>\n",
    "    \n",
    "1. Get attention scores by dot product: \n",
    "$ \\mathbf e_t = [s^T_t h_1, ..., s^T_t h_N] $\n",
    "    - Other alignment options available <br/>\n",
    "    <img src=\"https://i.stack.imgur.com/tiQkz.png\" width=\"300\"> \n",
    "    - Penalty available: penalize input tokens that have obtained high attention scores in past decoding steps \n",
    "    - $e'_t = e_t\\ if\\ t = 1\\ else\\ \\frac{exp(e_t)}{\\sum_{j=1}^{t-1}{exp(e_j)}} $ for decoder state\n",
    "    \n",
    "    \n",
    "4. Take softmax of $ \\mathbf e_t $ and get $ \\pmb\\alpha_t $ which sum up to one\n",
    "    - $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - Note: $\\pmb\\alpha_t$ can be interpreted as attention. For example, when generating word `vas`, the attention for `are` in encoder hidden states should be close to 1, others to 0<br/><br/>\n",
    "    \n",
    "    \n",
    "5. Take weighted sum of hidder states $\\mathbf h$ and $\\pmb\\alpha$, and get context vector **c**\n",
    "    - $ c_t = \\sum_{k=1}^{N} \\alpha_{tk}h_k $<br/><br/>\n",
    "    \n",
    "6. Generate *Attentional Hidden Layer*\n",
    "    - $ \\tilde h_t = tanh(W_c[c_t;h_t])$<br/><br/>\n",
    "\n",
    "7. Make Predicition\n",
    "    - $ p = softmax(W_s \\tilde h_t)$\n",
    "\n",
    "\n",
    "<img src=\"./figure/attention.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Mechanism\n",
    "\n",
    "**Main difference**\n",
    "\n",
    "1. Get attention scores by dot product: \n",
    "    - $ \\mathbf e_t = [s^T_{t-1} h_1, ..., s^T_{t-1} h_N] $<br/><br/>\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1}, c_t)$<br/><br/>\n",
    "    \n",
    "1. Make Predicition: \n",
    "    - $ p = softmax(g(s_t))$\n",
    "\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of two mechanism**\n",
    "\n",
    "<img src=\"http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "- RNN (LSTM): difficult to predict rare or out-of-vocabulary words\n",
    "- Pointer Network: generate word from input sentence (i.e., OoV - out of Vocabulary words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/efbd381493bb9636f489b965a2034d529cd56bcd/1-Figure1-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part I: Seq2Seq Attention Model\n",
    "    - See above\n",
    "    - $p_{vocabulary}(word)$\n",
    "    \n",
    "    \n",
    "- Part II: Pointer Generator\n",
    "    - After getting $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - $p_{pointer}(word) = \\sum \\alpha_t$, where position t is actually word w\n",
    "\n",
    "\n",
    "- Weighted sum: \n",
    "    - $g * p_{vocabulary}(word) + (1-g) * p_{pointer}(word) $\n",
    "    \n",
    "    \n",
    "- Applications:\n",
    "    - Summarization\n",
    "    - Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self/Intra/Inner Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute alignment function f among **decoder** hidden states $s_t$\n",
    "- Apply softmax for all states before current time $t$\n",
    "- Weighted sum will get current decoder attention output $c^d_t$\n",
    "- Why self-attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution\n",
    "## Coreference and Anaphora\n",
    "- Barak Obama travelled to,..., Obama\n",
    "- Obama says that he ....\n",
    "\n",
    "## Mention detection\n",
    "- Pronouns (I, your, it) - Part-of-Speech (POS) tagger\n",
    "- Named Entity (People name, place. tec) - NER system\n",
    "- Noun phrase (The dog stuck in the hole) - constituency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreferece model\n",
    "- Mention pair\n",
    "    * For each word, look at candidate antecedents, and train a **binary** classifier to predict $p(m_i,m_j)$\n",
    "\n",
    "- Mention rank\n",
    "    * Apply softmax to all candidate antecedents, and add highest scoring coreference link\n",
    "    * Each mention is only linked to **one** antecedent\n",
    "    \n",
    "\n",
    "- Clustering\n",
    "\n",
    "\n",
    "- Neural Coref Model\n",
    "    * Input layer: word embedding and other catogorical features (e.g., distance, document characteristic)\n",
    "<img src=\"./figure/Coref.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- End-to-end Model\n",
    "    * No separate mention detection step\n",
    "    * Apply LSTM and attention\n",
    "    * Consider span of text as a candidiate mention\n",
    "    * Final score: $s(i, j) = s_m(i) + s_m(j) + s_a(i, j)$, which means Is i, j mentions, and do they look coreferent.\n",
    "    \n",
    "<img src=\"./figure/endtoend.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
