{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://web.stanford.edu/~jurafsky/slp3/\n",
    "- http://www.davidsbatista.net/nlp_resources/\n",
    "- https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
    "\n",
    "\n",
    "# English Grammer\n",
    "- semantic: relating to meaning in language or logic\n",
    "- syntactic: of or according to syntax\n",
    "\n",
    "## Constituency and CFG\n",
    "- Constituents like noun-phrases\n",
    "- Constituent structure\n",
    "\n",
    "\n",
    "- Context-free Grammer (CFG)\n",
    "    - Consists of a set of **rules**, and a **lexicon** of words and symbols\n",
    "    - Define a **grammatical** sentence and perform **Syntactic Parsing**\n",
    "    - A (human) syntactically annotated corpus is called a **Treebank**.\n",
    "    - From **Treebank** we can extract CFG grammars \n",
    "    - A device to generate sentence\n",
    "    - A device to assign a structure to a sentence\n",
    "     <img src=\"./figure/grammer_lexicon.png\" width=\"400\">\n",
    "    <img src=\"./figure/grammer_rule.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- Example 1:\n",
    "    - NP → Det Nominal\n",
    "    - NP → ProperNoun\n",
    "    - Nominal → Noun | Nominal Noun\n",
    "    \n",
    "    - ***Finally***: Det + Nominal → a dog\n",
    "    - <img src=\"./figure/parse_tree.png\" width=\"100\">\n",
    "\n",
    "\n",
    "- Example 2: \n",
    "    - S → NP VP\n",
    "    - VP → Verb NP *e.g., prefer a morning flight*\n",
    "    - VP → Verb NP PP *e.g., leave Boston in the morning*\n",
    "    - VP → Verb PP *e.g., leaving on Thursday*\n",
    "    - PP → Preposition NP *e.g., from Los Angeles*\n",
    "    \n",
    "    \n",
    "- Example 3:\n",
    "    - <img src=\"./figure/parse_example.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar rules\n",
    "- Sentence-level construction\n",
    "    - Declarative: I want an apple\n",
    "    - Imperative: Show me the apple\n",
    "    - Yes/No question: Is this an apple?\n",
    "    - Wh structure: What flights do you have from Burbank to Tacoma Washington?\n",
    "        - S → Wh-NP Aux NP VP\n",
    "        - Long-distance dependencies -> Sometimes need semantic modelling\n",
    "- Other grammar rules TBA\n",
    "    - Noun. phrase\n",
    "    - Verb. phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "\n",
    "## sentence segmentation\n",
    "- Input: paragraph\n",
    "- Output: sentences\n",
    "- Ambiguity\n",
    "    - Period: whether end of sentence or Mr., Inc., etc.\n",
    "\n",
    "## word tokenization / text normalization\n",
    "- Input: sentence\n",
    "- Output: tokens\n",
    "    - contraction: is not --> isn't\n",
    "    - punctuation marks\n",
    "- Lemmatization:\n",
    "    - is, am, are → be\n",
    "    - He is reading detective stories\n",
    "    - He be read detective story.\n",
    "    \n",
    "- (**Simplified)** Stemming:\n",
    "    - ATIONAL → ATE (e.g., relational → relate)\n",
    "    - ING → \u000f if stem contains vowel (e.g., motoring → motor)\n",
    "    - SSES → SS (e.g., grasses → grass)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic parsing\n",
    "## Structural Ambiguity\n",
    "- Attachment Ambiguity\n",
    "    - <img src=\"./figure/structure_am.png\" width=\"500\">\n",
    "- Coordination Ambiguity\n",
    "    - [old [men and women]] or [old men] and [women] \n",
    "\n",
    "\n",
    "##  CKY Algorithm\n",
    "- Convet context-free grammar into Chomsky Normal Form (CFG->CNF)\n",
    "- <img src=\"./figure/cky.png\" width=\"500\">\n",
    "- <img src=\"./figure/cky2.png\" width=\"500\">\n",
    "\n",
    "## Partial parsing and chunking\n",
    "- Partial parsing: export flatter trees\n",
    "- Chunking: export **flat**, **non-overlapping** segments that constitute the basic non-recursive phrases corresponding to the major content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and prepositional phrases\n",
    "    - Example: [NP a flight] [PP from] [NP Indianapolis][PP to][NP Houston][PP on][NP\n",
    "NWA]\n",
    "- State-of-the-art chunking: ML\n",
    "    - Modelled as a classification problem\n",
    "    - <img src=\"./figure/chunking.png\" width=\"500\">\n",
    "    \n",
    "## Challenge\n",
    "- Is able to represent/interpret structural ambiguity, but cannot resolve them\n",
    "- Needs a methods to compute the probability of each interpretation and choose the most probable interpretation. (***Statistical Parsing***)\n",
    "\n",
    "\n",
    "# Statistical parsing\n",
    "***Probabilistic context-free grammar (PCFG)***\n",
    "- For rules defined from non-terminal node to child nodes, assign a probability (hopefully from a tree bank)$p$ \n",
    "- <img src=\"./figure/pcfg.png\" width=\"500\">\n",
    "- By comparing the $P = \\prod_i p$, we can tell which one is more probable\n",
    "- Assumptions: \n",
    "    - Independence\n",
    "        - For example, NPs that are syntactic *subjects* are far more likely to be pronouns; In other words, the location of the node in the tree also counts\n",
    "    - Lack of Sensitivity to Lexical Dependencies\n",
    "        - For example: [dogs in houses] and [cats], cannot distinguish dog, house, cat; In other words, may have coordination ambiguity\n",
    "- <img src=\"./figure/pcfg2.png\" width=\"500\">\n",
    "- Usage in Language Modelling\n",
    "    - N-gram: can only model a couple words of context\n",
    "    - PCFG: capable of using whole context \n",
    "  \n",
    "  \n",
    "- Some other disadvantage:\n",
    "    - Rely on a good tagger (PoS, n., v., adj., etc)\n",
    "    - Non-standard, unseen structures\n",
    "    - Have to define grammer at the beginning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "- Comparison between \n",
    "**Constituency**, phrase structure grammar, context-free grammars (CFGs) and **Dependency** grammar\n",
    "\n",
    "<img src=\"./figure/depen_cons.png\" width=\"600\">\n",
    "\n",
    "- In dependency parsing, an associated set of **directed**,  binary grammatical relations that hold among the words.\n",
    "\n",
    "- Advantage: \n",
    "    - One link type vs. Multiple set of phrase orders; Good for language with **free word order**\n",
    "    - Provides an approximation of semantic relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dependency structure\n",
    "Argument of relations\n",
    "- Head\n",
    "- Dependent\n",
    "\n",
    "Kinds of (grammatical) relations:\n",
    "- ***Clausal*** Argument Relations\n",
    "- Nominal ***Modifier*** Relations\n",
    "\n",
    "Projectivity\n",
    "- Crossing arcs leads to non-projectivity\n",
    "\n",
    "Dependency Treebanks\n",
    "- Universal Dependencies treebanks\n",
    "\n",
    "\n",
    "## Methods for dependency parsing\n",
    "- Transition-based parsing: \n",
    "http://stp.lingfil.uu.se/~sara/kurser/5LN455-2014/lectures/5LN455-F8.pdf\n",
    "<img src=\"./figure/transition.png\" width=\"600\">\n",
    "\n",
    "- Discriminative Classification Problem:\n",
    "    - Features: \n",
    "        - Top of stack word, POS; \n",
    "        - First in buffer word, POS; etc.\n",
    "        - ...\n",
    "        \n",
    "    - Speed\n",
    "        - Fast linear time parsing $O(n)$\n",
    "        \n",
    "    - Greedy\n",
    "        - No guarantee for best parsing\n",
    "        \n",
    "    - Algorithm\n",
    "        - Decision Tree, SVM, etc.\n",
    "\n",
    "\n",
    "- Evaluation\n",
    "    - Compare each pair of wordswith true relations\n",
    "\n",
    "## Models based on  Deep Learning \n",
    "http://web.stanford.edu/class/cs224n/lectures/lecture7.pdf\n",
    "\n",
    "- Use vector embeddings for words, POS tags and dependency relations\n",
    "<img src=\"./figure/dependency_dl_2.png\" width=\"300\">\n",
    "<img src=\"./figure/dependency_dl.png\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frequency and tf-idf\n",
    "- Characterize a word based on words within a window-size\n",
    "<img src=\"./figure/word_bow.png\" width=\"600\">\n",
    "<img src=\"./figure/word_bow2.png\" width=\"600\">\n",
    "- Result: **Sparse** and **Long** vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word Embedding \n",
    "\n",
    "Motivation:\n",
    "- Short and Dense\n",
    "- Capture Synonym\n",
    "- Less params to avoid overfitting\n",
    "\n",
    "\n",
    "### Word2vec\n",
    "\n",
    "- Why other Options not working\n",
    "    * One-hot vectors (vocabulary list too big; No similarity measurement; how about new words)\n",
    "    * Co-currence vector (matrix given a certain window size, # of times two words are together)->Sparsity\n",
    "    * Singular Vector Decomposition (SVD) for cocurrence matrix (too expensive)\n",
    "    * Use a word's context to represent --> Word embedding\n",
    "    \n",
    "    \n",
    "    \n",
    "- Key Components\n",
    "    * Center word *c*, context word *o*\n",
    "    * Two vectors for each word *w*: $ v_w $ and $ u_w $. $\\theta$ contains all *u* and *v* (Input and Output Vector)\n",
    "    * For example: $ P( w_2|w_1 ) = P(w_2|w_1;  u_{w2}, v_{w1}, \\theta )$\n",
    "    * Loss Function: $ J(\\theta) = -\\frac{1}{T}\\sum_{t}\\sum_{m\\in window} P(w_{t+j}|w_t)$\n",
    "    * Calculate u*v for each word, and use softmax to derive probability\n",
    "      - $ P(O|C) = \\frac{exp(u_o^T v_c)}{\\sum_{w}exp(u_w^T v_c)} $  \n",
    "      - $w$ is entire vocabulary\n",
    "        \n",
    "    * After optimization for loss, get two vectors for each word. Combine or Use *u* or Use *v*\n",
    "    * Can also be learned through a logistic regression\n",
    "    \n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=\"600\">    \n",
    "    \n",
    "- Variation\n",
    "    * Skip-grams (SG):given center, predict context\n",
    "    * Countinous Bag of Words (CBOW):given bag of context, predict center\n",
    "    * Negative sampling (maximize p of actual context + minimize p of random context i.e. noise)\n",
    "    * GloVe: combine count-based and direct-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word (BoW) Model\n",
    "<img src=\"./figure/BOW.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency encoding: long tail of more signoficant words\n",
    "- One-hot encoding: lose information of difference between words\n",
    "- TF-IDF: captures frequency; eliminate stop-words effect\n",
    "- TF-IDF: $w_{i,j} = tf_{i,j} \\times log(\\frac{N}{df_i})$, where \n",
    "    - term i and document j\n",
    "    - $tf_i$ is number of documents containing term i\n",
    "    - $df_{ij}$ is number of ocurrences of term i in document j\n",
    "    - N is total number of documents\n",
    "  \n",
    "  \n",
    "- ***High dimension when vocabulary increases***\n",
    "- ***Cannot deal with synonym***\n",
    "\n",
    "\n",
    "## Matrix Decomposition\n",
    "- LSA (Latent semantic analysis)\n",
    "    - Or: **non-negative matrix factorization (NNMF)**\n",
    "    - Perform SVD on TF-IDF matrix\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\" width=\"500\">\n",
    "   <img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0613.png\" width=\"500\">\n",
    "    - Compare 2 terms\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box3.png\" width=\"200\">\n",
    "    - Compare 2 documents\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box4.png\" width=\"300\">\n",
    "- Overall problem of vectorization with BoW models\n",
    "    - cannot measure similarity when sharing no terms\n",
    "    - high dimensionality with big sparseness\n",
    "    - lose information on grammar, semantic meanings\n",
    "    \n",
    "- ***Hard to explain the physical meaning of SVD***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detailed example and practice: see `Word_Embedding.ipynb`\n",
    "\n",
    "- Neural network serves as a look up table\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/flowchart-PerceptronLookup.png\" width=400>\n",
    "\n",
    "\n",
    "- Illustration of CBOW model: Ref: http://building-babylon.net/tag/word2vec/\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/context.png\" width=\"600\">\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/cbow.png\" width=\"600\">\n",
    "\n",
    "- Anotjer illustration of CBOW model\n",
    "<img src=\"https://i.stack.imgur.com/sAvR9.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector similarity\n",
    "    - Cosine similarity: <img src=\"https://qph.fs.quoracdn.net/main-qimg-fd48a47fdc134d6fc9b58cd86fdf244b\" width=300>\n",
    "    \n",
    "- ***LM model: representation layer; need to be combined with learning layer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed length with lower dimensionality for a document\n",
    "- A paragraph vector is added. The paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model.\n",
    "- Implementation in `gensim`: `model = Doc2Vec(corpus, size=5, min_count=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*9tVCGDm-ytPydhtJWVx3Zw.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed application, see another notebook `yelp_nlp.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0610.png\" width=\"500\">\n",
    "<img src=\"https://s3.amazonaws.com/skipgram-images/LDA.png\" width= \"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "- Task Definition: \n",
    "    - Predict next word\n",
    "    - Assign probabilities to sequence of words\n",
    "- For example \n",
    "    - google search, spellig check, speed recognition, machine translation\n",
    "\n",
    "## N-gram model\n",
    "   - Prototype: $P(L = 3, e_1 = I, e_2 = am, e_3 = WS) = P(e_1 = I) \\times P(e_2 = am | e_1 = I) \\times P(e_3 = WS | e_1 = I, e_2 = am) \\times P(e_4 = EoS |e_1 = I, e_2 = am, e_3 = WS)$ \n",
    "       - NOTE: from beginning of sentence\n",
    "   - Advantage of N-gram: Using count of different length of grams as they shown in corpus\n",
    "   \n",
    "   \n",
    "   - For example: 2-gram (bigram). Calculate probs of \"I am\", \"am WS\", \"WS EoS\" instead of \"I am WS EoS\"\n",
    "       - $P(e_t|e^{t-1}_1) = P(e_t|e_{t-1}) $]\n",
    "       - $P(e_t|e_{t-1}) = \\frac{C(e_t, e_{t-1})}{C(e_{t-1})}$\n",
    "       - Can be generalized to 3-gram model\n",
    "       - <img src=\"./figure/2-gram.png\" width=\"400\">\n",
    "\n",
    "\n",
    "   - Main problem: **Sparsity**, some senetence may not appear in training set, joint probability will be zero (The same problem as Prototype)\n",
    "   - Fix by **smoothing**/**interpolation**: $P(e_t|e_{t-1}) = (1-\\alpha)P_{ML}(e_t|e_{t-1}) + \\alpha P_{ML}(e_t)$\n",
    "       - A combination of unigram and bigram to ensure P>0\n",
    "       - Variation: more grams, context-dependent alpha, etc\n",
    "   - Fix unknown words by adding a \"**unk**\" word\n",
    "       - Replace singletopns or words only appearing once/few times in training corpus with $unk$\n",
    "       - Pre-define a closed vocabulary list $V$, and convert words in training set and not in $V$ to be $unk$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NN Model with Fixed Window Size      \n",
    "   - No Sparsity problem (using input embedding M, so possible to traet similar words similarly during prediction)\n",
    "   - Model size reduced (Instead of learning all probs {a, b, c} X {A, B, C}, Neural network learn weights to represent the quadratic combination)\n",
    "   - Ability to skip a previous word\n",
    "   - BUT: X do not share weight, and how to decide window size\n",
    "\n",
    "\n",
    "<img src=\"./figure/nn_lm.png\" width=\"700\">\n",
    "<img src=\"./figure/nn_lm_learn_embed.png\" width=\"700\">\n",
    "\n",
    "### RNN Model\n",
    "   - Any sequence length will work\n",
    "   - Weights shared, model size doesn't increase\n",
    "   - BUT: computation is slow (why) and cannot access information from many steps back\n",
    "   - Others applications of RNN\n",
    "        * One-to-one: tagging each word\n",
    "        * many-to-one: sentiment analysis\n",
    "        * Encoder module: example: element-wise max of all hidden states -> as input for further NN model\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*q1wyldq3Nm5pT266eXdfzA.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of LM\n",
    "- Given 1) Test dataset, and 2) trained language model P with parameter $\\theta$\n",
    "- Log likelihood $log(E_{test};\\theta) = \\sum_{E\\in E_{test}}{log[P(E;\\theta)]}$\n",
    "- Perplexity: $ ppl(E_{test};\\theta) = exp(-log(E_{test};\\theta) / len(E_{test}))$\n",
    "    - ppl = 1: perfect\n",
    "    - ppl = Vocabulary size: random model\n",
    "    - ppl = +inf: worst model\n",
    "    - ppl = some value $v$: need to pick $v$ values on average to get the correct one\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0502.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word Window Classification\n",
    "\n",
    "- Difference with typical ML: learn both **W** and word vectors **x**.\n",
    "\n",
    "- Task definition: classify a word in its *Context Window*\n",
    "    * Advantage: Do not train single word: ambiguity\n",
    "    * Advantage: Do not just average over window: lose position information\n",
    "    * Get a vector X with length of 5d where 5 is window size and d is embedding size\n",
    "    * Predict y based on softmax of WX and minimize cross-entropy error\n",
    "    \n",
    "    \n",
    "- Example: NER (*Named Entity Recognition*)\n",
    "    * 'Museums in Paris are good\". Binary task: whether Paris is a *location* or not.\n",
    "    \n",
    "    \n",
    "- What happens for word embedding x:\n",
    "    * Updated just as weigh W\n",
    "    * Pushed into an area helpful for classification task\n",
    "    * Example: $X_{in}$ may be a sign for location\n",
    "\n",
    "\n",
    "- Some **disadvantages** of window-based methods\n",
    "    - Anything outside the context window is ignored\n",
    "    - Hard to learn systematic pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.stack.imgur.com/a6CJc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "- Concepts\n",
    "    - Closed class: \n",
    "        - Prepositions: on, in\n",
    "        - Particles: (turn sth) over\n",
    "        - Determiner: a, an, the, this, that\n",
    "        - Conjunctions: and, or, but\n",
    "        - Pronouns: my, your, who\n",
    "    - Open classes:\n",
    "        - Nouns\n",
    "        - Verbs\n",
    "        - Adjectives\n",
    "        - Adverbs\n",
    "        \n",
    "- Tagset:\n",
    "    - 45-tag Penn Treebank tagset\n",
    "    \n",
    "    \n",
    "- Training set\n",
    "    - Corpora labeled with parts-of-speech\n",
    "    \n",
    "- Ambiguity:\n",
    "    - Example: book (noun. or verb.)\n",
    "    - Solution1: Most Frequent Class Baseline: for example: $a$ is a determiner instead of a letter in most cases\n",
    "    \n",
    "\n",
    "## Hidden-Markov-Model (HMM) for PoS tagging\n",
    "\n",
    "### Prepare transion matrix from training examples\n",
    "- A matrix: tag transition matrix\n",
    "    - VB: verb, MD: modal verb like \"will\"\n",
    "    - $P(VB|MD) = \\frac{C(MD, VB)}{C(MD)} = 0.8$\n",
    "- B matrix: emission matrix\n",
    "    - $P(\"will\"|MD) = \\frac{C(MD, \"will\")}{C(MD)} = 0.3$\n",
    "    \n",
    "    \n",
    "<img src=\"./figure/Hmm.png\" width=\"600\"> \n",
    "### Define Optimal solution\n",
    "- Solve by NB\n",
    "- $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) \\\\\n",
    "\\xrightarrow{Bayesian} P(T_{1-n})P(W_{1-n}|T_{1-n}) \\\\\n",
    "\\xrightarrow{Inpendence\\ Assumption} P(T_{1-n}) \\prod_{i=1}^n P(W_i|T_i) \\\\\n",
    "\\xrightarrow{Bigram\\ Assumption}  \\prod_{i=1}^n P(T_i|T_{i-1}) \\prod_{i=1}^n P(W_i|T_i) \\\\\n",
    "=  \\prod_{i=1}^n P(T_i|T_{i-1})P(W_i|T_i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Algorithm\n",
    "- The Viterbi Algorithm\n",
    "    - Essentially dynamic programming problem\n",
    "    - See mini example for details: http://www.davidsbatista.net/assets/documents/posts/2017-11-11-hmm_viterbi_mini_example.pdf\n",
    "    - $\\delta_i(T)=max_{T_{i−1}}  P(T|T_{i−1})⋅P(W_{i−1}∣T_{i−1})⋅\\delta_i(T_{i−1})$\n",
    "    - <img src=\"./figure/Viterby.png\" width=\"400\">\n",
    "    - Note: if previoys state is fixed, it is equivalent to NB\n",
    "\n",
    "\n",
    "\n",
    "- Beam search\n",
    "    - Better computational efficiency\n",
    "    \n",
    "<img src=\"./figure/bs.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions:\n",
    "- Modify Bigram Assumption to be Trigram Assumption: $\\prod_{i=1}^n P(T_i|T_{i-1}, T_{i-2})P(W_i|T_i)$\n",
    "- Add awareness of sentence end: $[\\prod_{i=1}^n P(T_i|T_{i-1}, T_{i-2})P(W_i|T_i)]\\ P(T_{n+1}|T_n)$\n",
    "- Data interpolations to fix sparseness: similar to smoothing in n-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Markov-Model (MEMM)\n",
    "<img src=\"./figure/hmm_memm.png\" width=\"600\">\n",
    "-  HMM/Generative: $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) \\\\\n",
    "\\xrightarrow{Bayesian} P(T_{1-n})P(W_{1-n}|T_{1-n}) =  \\prod_{i=1}^n P(T_i|T_{i-1})P(W_i|T_i)$ \n",
    "\n",
    "\n",
    "- MEMM/Discriminative: $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) = \\prod_{i=1}^n P(T_i|T_{i-1}, W_i)$\n",
    "\n",
    "<img src=\"./figure/memm.png\" width=\"600\">\n",
    "<img src=\"./figure/memm_fea_tmp.png\" width=\"300\">\n",
    "\n",
    "\n",
    "- (***For unknown words***) Ability to contain feature sets besides simply $w_i$ and $t_{i-1}$.\n",
    "    - wi contains a particular prefix (from all prefixes of length ≤ 4)\n",
    "    - wi contains a particular suffix (from all suffixes of length ≤ 4)\n",
    "    - wi contains a number\n",
    "    - wi contains an upper-case letter\n",
    "    - wi contains a hyphen\n",
    "    - prefix\n",
    "    - suffix\n",
    "    - ......\n",
    "    \n",
    "- Solution Algorithm: Viterbi algorithm just as with the HMM\n",
    "\n",
    "- Main Advantages:\n",
    "    - Capability of including more features (compared with **HMM**)\n",
    "\n",
    "\n",
    "- Main drawback\n",
    "    - Label Bias Problem: MEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n",
    "    - <img src=\"./figure/LBP.png\" width=\"500\">\n",
    "    - **BUT**: State 2 may NEVER see $i$\n",
    "\n",
    "\n",
    "- Improvement\n",
    "    - Replace: local per state normalization\n",
    "    - By: global per sequence normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectionality\n",
    "- Conditional random field or CRF model\n",
    "- Global Normalization\n",
    "- <img src=\"http://www.davidsbatista.net/assets/images/2017-11-13-CRF_Equation.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical types of entity\n",
    "<img src=\"./figure/ner_type.png\" width=\"600\">\n",
    "\n",
    "- When ambiguity arises\n",
    "<img src=\"./figure/ner_ambiguity.png\" width=\"600\">\n",
    "\n",
    "## Modelling Approach\n",
    "- Features\n",
    "    - identity of $w_{i-1}, w_i,w_{i+1}$\n",
    "    - embeddings for $w_{i-1}, w_i,w_{i+1}$\n",
    "    - part of speech of $w_{i-1}, w_i,w_{i+1}$\n",
    "\n",
    "    - base-phrase syntactic chunk label of $w_{i-1}, w_i,w_{i+1}$\n",
    "    - $w_i$ contains a particular prefix (from all prefixes of length ≤ 4)\n",
    "    - $w_i$ contains a particular suffix (from all suffixes of length ≤ 4)\n",
    "    - $w_i$ is all upper case\n",
    "    - word shape of $w_{i-1}, w_i,w_{i+1}$\n",
    "        - for example: $DC10-30$ would map to $XXdd-dd$\n",
    "\n",
    "- Model\n",
    "<img src=\"./figure/ner_model.png\" width=\"600\">\n",
    "    - Rule-basded models\n",
    "    - Deep-learning models\n",
    "        - <img src=\"./figure/ner_DL.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affect Lexicon\n",
    "- **Emotions** is one of the different dimensions for affect lexicons, like emotions, attitudes, personalities, etc.\n",
    "\n",
    "- **Sentiment**: can be viewed as a special case of emotions in the valence dimension, measuring how pleasant or unpleasant a word is.\n",
    "\n",
    "\n",
    "- How to create sentiment lexicon (Lexicon Induction Methods)\n",
    "    - Human labelling\n",
    "    - Semi-supervised (Semantic axis methods)\n",
    "        - Start from seed words that define 2 poles (good vs. bad)\n",
    "        - Measure the similarity of new owrd $w$ with seed words\n",
    "        - Word vectors can coming from fine-tuned off-the-shelf word embeddings\n",
    "        - Cosine similarity\n",
    "    - Supervisef\n",
    "        - Based on review scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prepare\n",
    "- Evaluation Metrics\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - Accuracy\n",
    "    - F1\n",
    "    - AUC\n",
    "    \n",
    "    \n",
    "- Data collection:\n",
    "    - Web scrapping, crawl from internet\n",
    "    - Human annotation\n",
    "    - Data balancing\n",
    "\n",
    "\n",
    "- Prepare dictionary\n",
    "    - Emotion icons (human labels), replace with text\n",
    "    - Acronym dictionary (lol), replace with full form\n",
    "\n",
    "### Model \n",
    "\n",
    "- Baseline Model\n",
    "    - Lexicon-based method: Count +/- words\n",
    "        - Example from *MPQA lexicon*\n",
    "        - \\+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great\n",
    "        - \\− : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\n",
    "\n",
    "\n",
    "- ***Text Mining Features***\n",
    "    - Preprocessing\n",
    "        - Remove stopword\n",
    "        - Remove tags\n",
    "        - Remove urls\n",
    "        - Lemmatization\n",
    "        - Stemming\n",
    "        - Tokenization\n",
    "    - Unigram/Bigram (I go out, (I go, go out))\n",
    "        - Bigram addressed the problem of only considering single word, but larger vector\n",
    "        - Add negation:\n",
    "            - Example: did**n’t** like this movie , but I\n",
    "            - Becomes: didn’t **NOT_like** **NOT_this** **NOT_movie** , but I\n",
    "    - Bag of Word \n",
    "    - TF-IDF \n",
    "    - **Problem**: dimension too big\n",
    "\n",
    "\n",
    "- Naive Bayes  (Calculate $P(k), P(X|k)$)\n",
    "    - <img src=\"./figure/NB_al.png\" width=\"300\">\n",
    "    -  Naïve Bayes classifiers assume that the effect of a variable value on a given class is independent of the values of other variable. This assumption is called class conditional independence.\n",
    "    - ***Good for small dataset, and easy to train***\n",
    "    - Numerical Example:\n",
    "    <img src=\"./figure/NB_example.png\" width=\"400\">\n",
    "    - P(+) =, P(-) = \n",
    "    - P(predictable|+) = ...\n",
    "    - P(predictable|-) = ...\n",
    "    - P(+)P(S|+) = ...\n",
    "    - P(-)P(S|-) = ...\n",
    "    \n",
    "    - Some improvements for sentiment analysis:\n",
    "        - Code word appearance instead of frequency\n",
    "\n",
    "\n",
    "\n",
    "- ***Environment Features***\n",
    "    - Location\n",
    "    - Time\n",
    "    - Device\n",
    "    - .....\n",
    "\n",
    "\n",
    "- ***Linguistic Features***\n",
    "    - **Human efforts involved**\n",
    "    - Length of comment\n",
    "    - Number of negative words\n",
    "    - Sum of prior scores\n",
    "    - Number of hashtags\n",
    "    - Number of handles\n",
    "    - Repeat characters\n",
    "    - Exclamation, Capitalized tesxt\n",
    "    - Number of new lines\n",
    "        \n",
    "<img src=\"./figure/logistic_feature.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- Comparison between NB and LR\n",
    "    - NB: Strong conditional independence assumption\n",
    "    - LR: better estimate correlated features\n",
    "    - LR: add regularizations:\n",
    "        - L1: Laplace Prior\n",
    "        - L2: Gaussian Distribution with zero meam\n",
    "        - Bayesian interpretations: ${argmax}_w [P(Y|w,x)P(w)] = {argmax}_w [Log(P(Y|w,x)) - \\alpha \\sum w^2]$ when $P(w)$ is zero mean Gaussian\n",
    "\n",
    "\n",
    "\n",
    "- Machine Learning Algorithm\n",
    "    - SVM\n",
    "    - DT/RF/BT\n",
    "    \n",
    "\n",
    "- Deep Learning Algorithm\n",
    "    - Set sequence length\n",
    "    - Set vocabulary size\n",
    "    - Word embedding\n",
    "        - Pre-trained (GloVe)\n",
    "        - Online-traning\n",
    "    - CNN\n",
    "    - LSTM\n",
    "        - E.g., word \"not\" can be rotating the polarity of the next word\n",
    "      \n",
    "      \n",
    "- Syntactic meanings\n",
    "    - Dependency parsing\n",
    "    - Coreference\n",
    "    - Sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguity\n",
    "https://slideplayer.com/slide/5187871/\n",
    "\n",
    "Background Example:\n",
    "- **Lemma**: Mouse, has 2 **senses**\n",
    "    1. any of numerous small rodents...\n",
    "    2. a hand-operated device that controls a cursor...\n",
    "\n",
    "\n",
    "- Words with the same **sense**: ***Synonyms***\n",
    "- Words with the opposite **senses**: ***Antonym***\n",
    "- Words with similarity: similar words (cat, dog)\n",
    "- Words with relatedness: (coffee, cup, waiter, menu, plate,food, chef), they belong to the same **senamtic field**\n",
    "- Words with taxonomic relations: we say that vehicle is a **hypernym** of car, and dog is a **hyponym** of animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "- Just pick most frequent sense\n",
    "\n",
    "\n",
    "### Knowledge-Based\n",
    "\n",
    "***How to define similarity***\n",
    "- Each word has gloss, example, etc.\n",
    "- Lesk Algorithm: $Score\\ (sense_i, context_j) = similarity\\ (example_i, context_j)$. For example: bank vs. deposits\n",
    "- Similarity can be defined by, e.g., percent of overlapping words\n",
    "\n",
    "\n",
    "***Pro/Con***\n",
    "- One model for all\n",
    "- Can use similar words / synosym if example is limited\n",
    "- These algorithms are overlap based, so they suffer from overlap sparsity and performance depends on dictionary definitions.\n",
    "\n",
    "\n",
    "### Supervised \n",
    "\n",
    "***How to featurize a sample text***\n",
    "- Collocational features: Position-specific information\n",
    "    - *\"guitar and --bass-- player stand\"*\n",
    "    - Feature: POS tag for targets and neighbors, and context words: $[w_{i-2}, POS_{i-2}...,w_{i+2}, POS_{i+2} ]$\n",
    "\n",
    "\n",
    "- Other syntactic features of the sentence\n",
    "    - Passive or not\n",
    "\n",
    "\n",
    "- BoW features\n",
    "    - Vocabulary list: [[fishing,\tbig,\tsound,\tplayer,\tfly,\trod,\tpound,\tdouble,\truns,\tplaying,\tguitar,\tband]\t]\n",
    "    - Feature: [0,0,0,1,0,0,0,0,0,0,1,0]\t\n",
    "    \n",
    "***Apply classfication algorithms***\n",
    "- NB\n",
    "- SVM\n",
    "\n",
    "\n",
    "***Pro/Con***\n",
    "- This type of algorithms are better than the two approaches w.r.t.implementation perspective.\n",
    "- These algorithms don’t give satisfactory result for resource scarce languages. \n",
    "- Need to train it for each word\n",
    "\n",
    "### Semi-supervised\n",
    "- Bootstrapping\n",
    "\n",
    "### Unsupervised\n",
    "- Word-sense induction\n",
    "- Topic modelling\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0601.png\" width= \"500\">\n",
    "\n",
    "- Some key points\n",
    "    - Definition of distance: cosine distance, Euclidean distance\n",
    "    - Vectorization: one-hot, TF-IDF\n",
    "    - Algorithm: K-means, Agglomerative clustering\n",
    "    - Number of clusters $K$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Matching\n",
    "### Deep Structured Semantic Models\n",
    "<img src=\"https://raw.githubusercontent.com/v-liaha/v-liaha.github.io/master/assets/dssm.png\" width=\"800\">\n",
    "​\n",
    "To Read:\n",
    "https://cloud.tencent.com/developer/article/1173704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation\n",
    "\n",
    "## Problem definition\n",
    "- Neural Machine Translation (NMT)\n",
    "- Sequence-to-Sequence(seq2seq) architecture\n",
    "- Difference from SMT (Statistical MT): calculate P(y|x) directly instead of using Bayes\n",
    "- Advantage: Single NN, less human engineering\n",
    "- Disadvantage: less interpretable, less control\n",
    "\n",
    "## Main Components\n",
    "- Encoder RNN: encode source sentence, generate hidden state\n",
    "- Decoder RNN: **Language Model**, generate target sentence using outputs from encoder RNN; predict next word in *y* conditional on input *x*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1585/1*sO-SP58T4brE9EHazHSeGA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allow information from future inputs\n",
    "* LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google’s neural machine translation (Google-NMT) \n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0109.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "\n",
    "- Greedy decoding problem\n",
    "    * Instead of generating argmax each step, use beam search.\n",
    "    * Keep *k* most probable translations\n",
    "    * Exactly *k* nodes at each time step *t*\n",
    "    * *Note*: Length bias, prefer shorter sentence because the log(P) accumulates. Can add prior for sentence length to compensate.\n",
    "    \n",
    "<img src=\"./figure/beam.png\" width=\"300\">\n",
    "https://arxiv.org/pdf/1703.01619.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention model\n",
    "<img src=\"https://miro.medium.com/max/1910/1*wnXVyE8LXPfODvB_Z5vu8A.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Advantage\n",
    "\n",
    "- Focus on certain parts of source (Instead of encoding whole sentence in **one single** hidden vector, here the sentence is coded as whole input **sequence**.)\n",
    "- Provides shortcut / Bypass bottleneck\n",
    "- Get some interpretable results and learn alignment\n",
    "- \"Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM\"\n",
    "\n",
    "<img src=\"./figure/chinese_nlp.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong attention mechanism\n",
    "\n",
    "\n",
    "\n",
    "1. Get ***encoder*** hidden states: $ h_1, ..h_k,..., h_N $\n",
    "\n",
    "1. Get ***decoder*** hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1})$<br/><br/>\n",
    "    \n",
    "1. Get attention scores by dot product: \n",
    "$ \\mathbf e_t = [s^T_t h_1, ..., s^T_t h_N] $\n",
    "    - Other alignment options available <br/>\n",
    "    <img src=\"https://i.stack.imgur.com/tiQkz.png\" width=\"300\"> \n",
    "    - Penalty available: penalize input tokens that have obtained high attention scores in past decoding steps \n",
    "    - $e'_t = e_t\\ if\\ t = 1\\ else\\ \\frac{exp(e_t)}{\\sum_{j=1}^{t-1}{exp(e_j)}} $ for decoder state\n",
    "    \n",
    "    - With FC layer: <img src=\"https://miro.medium.com/max/1364/1*wxv56cPyJdrEFSkknrlP-A.jpeg\" width=\"400\">\n",
    "    <img src=\"https://miro.medium.com/max/1902/1*jRBjCcGSoVL-rDb_zBXyPQ.jpeg\" width=\"400\">\n",
    "4. Take softmax of $ \\mathbf e_t $ and get $ \\pmb\\alpha_t $ which sum up to one\n",
    "    - $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - Note: $\\pmb\\alpha_t$ can be interpreted as attention. For example, when generating word `vas`, the attention for `are` in encoder hidden states should be close to 1, others to 0<br/><br/>\n",
    "    \n",
    "    \n",
    "5. Take weighted sum of hidder states $\\mathbf h$ and $\\pmb\\alpha$, and get context vector **c**\n",
    "    - $ c_t = \\sum_{k=1}^{N} \\alpha_{tk}h_k $<br/><br/>\n",
    "    \n",
    "6. Generate *Attentional Hidden Layer*\n",
    "    - $ \\tilde h_t = tanh(W_c[c_t;s_t])$<br/><br/>\n",
    "\n",
    "7. Make Predicition\n",
    "    - $ p = softmax(W_s \\tilde h_t)$\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Depiction-of-Global-Attention-in-an-Encoder-Decoder-Recurrent-Neural-Network.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Mechanism\n",
    "\n",
    "**Main difference**\n",
    "\n",
    "1. Get attention scores by dot product: \n",
    "    - $ \\mathbf e_t = [s^T_{t-1} h_1, ..., s^T_{t-1} h_N] $<br/><br/>\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1}, c_t)$<br/><br/>\n",
    "    \n",
    "1. Make Predicition: \n",
    "    - $ p = softmax(g(s_t))$\n",
    "\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another way to illustrate:\n",
    "    - Step 1: <img src=\"https://miro.medium.com/max/1914/1*IT-_Z0arAHdRnbf4T-BUKw.jpeg\" width=\"500\"> \n",
    "    - Step 2: <img src=\"https://miro.medium.com/max/1908/1*52xHMRpOX_88hhrtQ70zPw.jpeg\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of two mechanism**\n",
    "\n",
    "<img src=\"http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png\" width=\"800\">\n",
    "\n",
    "**Example of attention weights**\n",
    "<img src=\"https://i.stack.imgur.com/WxG8e.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "- RNN (LSTM): difficult to predict rare or out-of-vocabulary words\n",
    "- Pointer Network: generate word from input sentence (i.e., OoV - out of Vocabulary words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/efbd381493bb9636f489b965a2034d529cd56bcd/1-Figure1-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part I: Seq2Seq Attention Model\n",
    "    - See above\n",
    "    - $p_{vocabulary}(word)$\n",
    "    \n",
    "    \n",
    "- Part II: Pointer Generator\n",
    "    - After getting $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - $p_{pointer}(word) = \\sum \\alpha_t$, where position t is actually word w\n",
    "\n",
    "\n",
    "- Weighted sum: \n",
    "    - $g * p_{vocabulary}(word) + (1-g) * p_{pointer}(word) $\n",
    "    \n",
    "    \n",
    "- Applications:\n",
    "    - Summarization\n",
    "    - Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self/Intra/Inner Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why do we need self-attention\n",
    "    - As the model processes each word (**each position** in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for **this word**.Now it is **Bi-directional** instead of single-directional\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- How to calculate?\n",
    "    - By having Q / K / V matrice\n",
    "    - <img src=\"http://jalammar.github.io/images/t/self-attention-output.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- How to incorporate multiple heads (i.e., multiple self attention mechanisms / representation subspaces? \n",
    "    - Motivation: having different attentions based on type of questions, like who / what / when.\n",
    "    - By having multiple Q / K / V weight matrix\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"500\">\n",
    "    - <img src=\"https://miro.medium.com/max/1232/1*8H6TqcfHrtNCc9_Qva7xog.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose\n",
    "    - ***Neural Machine Translation***, Speech Recognition, text-to-speech recognition\n",
    "    - The Transformers outperforms the *Google Neural Machine Translation model* in specific tasks.\n",
    "- Motivations\n",
    "    - RNN: information loss along the chain (long-term dependency problem)\n",
    "    - LSTM: \n",
    "        - still have information loss for long sentence, because distance between positions is *linear* (i.e., when the distance is increasing, probability of keep the context is lower) \n",
    "        - cannot be parallelized (computation is sequential)\n",
    "    - RNN with Attention:\n",
    "        - code sentence as sequence of hidder states instead of a single one\n",
    "        - focus on difference positions at each time step\n",
    "        - still cannot be parallelized\n",
    "    - CNN:\n",
    "        - can be parallezied (distance between input and output is on the order of $log(N)$.\n",
    "        - cannot figure out the problem of dependencies\n",
    "    - Transformer:\n",
    "        - self-attenton layer: have dependencies (i.e., access to all positions, non-directional)\n",
    "        - fc layer: have no dependencies and can be executed in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- How to acccount for order of words? / Distnace between words\n",
    "    - By having position encodings\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- How the encoder looks like:\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" width=\"400\">\n",
    "  \n",
    "  \n",
    "- How the decoder looks like:\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" width=\"600\">\n",
    "    -  Difference from encoder 1: In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "    - Difference from encoder 2: The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
    "\n",
    "\n",
    "- How the output looks like:\n",
    "    - Using typical softmax layer\n",
    "    - <img src=\"http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional Encoder Representations from Transformers\n",
    "    - Goal: generate a language model (LM)\n",
    "    - Motivation: transfer learning in image classification\n",
    "    - Approach: only use the encoder mechanism from transformer\n",
    "    - Comparison with Wordvec/Glove:\n",
    "        - In wordvec, each word has single embedding (e.g., bank is a financial institute vs, bank of the river) - ***context-independent***. Also note that **word order** is ignored during training.\n",
    "        - In bert: the whole ***context*** is considered to generate the vector\n",
    "    - Use cases:\n",
    "        - Classification\n",
    "        - Question Answering\n",
    "        - NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How the BERT model is trained?\n",
    "    - Task 1: Masked LM (MLM) - Masking 15% of the word as *MASK*.\n",
    "    - The goal is the predict the masked words based on unmasked words.\n",
    "    - <img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\" width=\"500\">\n",
    "    \n",
    "    - Task 2: Next Sentence Prediction (NSP)\n",
    "    - During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence.\n",
    "    - <img src=\"https://miro.medium.com/max/1174/0*m_kXt3uqZH9e7H4w.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval (IR) -based Factoid Question Answering\n",
    "<img src=\"./figure/IR_QA.png\" width=\"600\">\n",
    "\n",
    "***Query Formulation***\n",
    "- Input: Question Text\n",
    "- Output: [***Tokens***] sent to IR system\n",
    "- Example: \n",
    "    - Before: when was the laser invented\n",
    "    - After: the laser was invented\n",
    "- Model:\n",
    "    - Human rules\n",
    "    \n",
    "***Question-Type Classifier***\n",
    "- Input: question text\n",
    "- Output: NER or predefined category\n",
    "- Example:\n",
    "    - who {is | was | are | were} **PERSON**\n",
    "    \n",
    "- Model: supervised-learning\n",
    "    - Features:\n",
    "        - word embeddings\n",
    "        - PoS tags\n",
    "        - NERs in question text\n",
    "     - Often: Answer type word\n",
    "         - Which **City**\n",
    "         - What **Time**\n",
    "\n",
    "***Retrieve Documents***\n",
    "- Initial Ranking\n",
    "    - Based on similarity/Relevance to query\n",
    "\n",
    "- Further Ranking\n",
    "    - Supervised Learning\n",
    "    - Features:\n",
    "        - The number of named entities of the right type in the passage\n",
    "        - The number of question keywords in the passage\n",
    "        - The longest exact sequence of question keywords that occurs in the passage\n",
    "        - The rank of the document from which the passage was extracted\n",
    "        - The proximity of the keywords from the original query to each other \n",
    "        - The number of n-grams that overlap between the passage and the question\n",
    "        \n",
    "        \n",
    "***Answer Extraction***\n",
    "- Baseline: \n",
    "    - Run NER tagging, and return span that matches the question-type\n",
    "    - For example:\n",
    "        - “How tall is Mt. Everest?” [**DISTANCE**]\n",
    "        - The official height of Mount Everest is **29029 feet**\n",
    "        \n",
    " \n",
    "- Supervised-Learning\n",
    "    - Goal: to determine if a span/sentence contains answer\n",
    "    - Feature-based\n",
    "        - Answer type match\n",
    "        - Number of matched question keywords\n",
    "        \n",
    "    - Deep learing\n",
    "        - Example: question: *“**When** did Beyonce release Dangerously in Love?*\n",
    "        - Example Answer: *Their hiatus saw the release of Beyonce’s debut album, Dangerously in Love **(2003)**, which established...*\n",
    "        - <img src=\"./figure/QA.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge-based Question Answering\n",
    "\n",
    "- Extract information from structured database\n",
    "- Examples\n",
    "    - “When was Ada Lovelace born?” → **birth-year (Ada Lovelace, ?x)**\n",
    "    - “What is the capital of England?” → **capital-city(?x, England)**\n",
    "\n",
    "- General Approach\n",
    "    - Parse question text\n",
    "    - Align parsed trees to logical forms\n",
    "    - <img src=\"./figure/knowledge-based.png\" width=\"600\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution\n",
    "## Coreference and Anaphora\n",
    "- Barak Obama travelled to,..., Obama\n",
    "- Obama says that he ....\n",
    "\n",
    "## Mention detection\n",
    "- Pronouns (I, your, it) - Part-of-Speech (POS) tagger\n",
    "- Named Entity (People name, place. tec) - NER system\n",
    "- Noun phrase (The dog stuck in the hole) - constituency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreferece model\n",
    "- Mention pair\n",
    "    * For each word, look at candidate antecedents, and train a **binary** classifier to predict $p(m_i,m_j)$\n",
    "\n",
    "- Mention rank\n",
    "    * Apply softmax to all candidate antecedents, and add highest scoring coreference link\n",
    "    * Each mention is only linked to **one** antecedent\n",
    "    \n",
    "\n",
    "- Clustering\n",
    "\n",
    "\n",
    "- Neural Coref Model\n",
    "    * Input layer: word embedding and other catogorical features (e.g., distance, document characteristic)\n",
    "<img src=\"./figure/Coref.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- End-to-end Model\n",
    "    * No separate mention detection step\n",
    "    * Apply LSTM and attention\n",
    "    * Consider span of text as a candidiate mention\n",
    "    * Final score: $s(i, j) = s_m(i) + s_m(j) + s_a(i, j)$, which means Is i, j mentions, and do they look coreferent.\n",
    "    \n",
    "<img src=\"./figure/endtoend.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/07/comparison-top-6-python-nlp-libraries.html\n",
    "<img src=\"https://activewizards.com/content/blog/Comparison_of_Python_NLP_libraries/nlp-librares-python-prs-and-cons01.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "469px",
    "left": "0px",
    "right": "896.646px",
    "top": "107px",
    "width": "241px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
