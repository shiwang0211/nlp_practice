{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "https://web.stanford.edu/~jurafsky/slp3/\n",
    "# English Grammer\n",
    "- semantic: relating to meaning in language or logic\n",
    "- syntactic: of or according to syntax\n",
    "\n",
    "## Constituency and CFG\n",
    "- Constituents like noun-phrases\n",
    "- Constituent structure\n",
    "\n",
    "\n",
    "- Context-free Grammer (CFG)\n",
    "    - Consists of a set of **rules**, and a **lexicon** of words and symbols\n",
    "    - Define a **grammatical** sentence and perform **Syntactic Parsing**\n",
    "    - A (human) syntactically annotated corpus is called a **Treebank**.\n",
    "    - From **Treebank** we can extract CFG grammars \n",
    "    - A device to generate sentence\n",
    "    - A device to assign a structure to a sentence\n",
    "     <img src=\"./figure/grammer_lexicon.png\" width=\"400\">\n",
    "    <img src=\"./figure/grammer_rule.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- Example 1:\n",
    "    - NP → Det Nominal\n",
    "    - NP → ProperNoun\n",
    "    - Nominal → Noun | Nominal Noun\n",
    "    \n",
    "    - ***Finally***: Det + Nominal → a dog\n",
    "    - <img src=\"./figure/parse_tree.png\" width=\"100\">\n",
    "\n",
    "\n",
    "- Example 2: \n",
    "    - S → NP VP\n",
    "    - VP → Verb NP *e.g., prefer a morning flight*\n",
    "    - VP → Verb NP PP *e.g., leave Boston in the morning*\n",
    "    - VP → Verb PP *e.g., leaving on Thursday*\n",
    "    - PP → Preposition NP *e.g., from Los Angeles*\n",
    "    \n",
    "    \n",
    "- Example 3:\n",
    "    - <img src=\"./figure/parse_example.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar rules\n",
    "- Sentence-level construction\n",
    "    - Declarative: I want an apple\n",
    "    - Imperative: Show me the apple\n",
    "    - Yes/No question: Is this an apple?\n",
    "    - Wh structure: What flights do you have from Burbank to Tacoma Washington?\n",
    "        - S → Wh-NP Aux NP VP\n",
    "        - Long-distance dependencies -> Sometimes need semantic modelling\n",
    "- Other grammar rules TBA\n",
    "    - Noun. phrase\n",
    "    - Verb. phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "\n",
    "## sentence segmentation\n",
    "- Input: paragraph\n",
    "- Output: sentences\n",
    "- Ambiguity\n",
    "    - Period: whether end of sentence or Mr., Inc., etc.\n",
    "\n",
    "## word tokenization / text normalization\n",
    "- Input: sentence\n",
    "- Output: tokens\n",
    "    - contraction: is not --> isn't\n",
    "    - punctuation marks\n",
    "- Lemmatization:\n",
    "    - is, am, are → be\n",
    "    - He is reading detective stories\n",
    "    - He be read detective story.\n",
    "    \n",
    "- (**Simplified)** Stemming:\n",
    "    - ATIONAL → ATE (e.g., relational → relate)\n",
    "    - ING → \u000f if stem contains vowel (e.g., motoring → motor)\n",
    "    - SSES → SS (e.g., grasses → grass)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic parsing\n",
    "## Structural Ambiguity\n",
    "- Attachment Ambiguity\n",
    "    - <img src=\"./figure/structure_am.png\" width=\"500\">\n",
    "- Coordination Ambiguity\n",
    "    - [old [men and women]] or [old men] and [women] \n",
    "\n",
    "\n",
    "##  CKY Algorithm\n",
    "- Convet context-free grammar into Chomsky Normal Form (CFG->CNF)\n",
    "- <img src=\"./figure/cky.png\" width=\"500\">\n",
    "- <img src=\"./figure/cky2.png\" width=\"500\">\n",
    "\n",
    "## Partial parsing and chunking\n",
    "- Partial parsing: export flatter trees\n",
    "- Chunking: export **flat**, **non-overlapping** segments that constitute the basic non-recursive phrases corresponding to the major content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and prepositional phrases\n",
    "    - Example: [NP a flight] [PP from] [NP Indianapolis][PP to][NP Houston][PP on][NP\n",
    "NWA]\n",
    "- State-of-the-art chunking: ML\n",
    "    - Modelled as a classification problem\n",
    "    - <img src=\"./figure/chunking.png\" width=\"500\">\n",
    "    \n",
    "## Challenge\n",
    "- Is able to represent/interpret structural ambiguity, but cannot resolve them\n",
    "- Needs a methods to compute the probability of each interpretation and choose the most probable interpretation. (***Statistical Parsing***)\n",
    "\n",
    "# Statistical parsing\n",
    "***Probabilistic context-free grammar (PCFG)***\n",
    "- For rules defined from non-terminal node to child nodes, assign a probability (hopefully from a tree bank)$p$ \n",
    "- <img src=\"./figure/pcfg.png\" width=\"500\">\n",
    "- By comparing the $P = \\prod p$, we can tell which one is more probable\n",
    "- Assumptions: \n",
    "    - Independence\n",
    "        - For example, NPs that are syntactic *subjects* are far more likely to be pronouns; In other words, the location of the node in the tree also counts\n",
    "    - Lack of Sensitivity to Lexical Dependencies\n",
    "        - For example: [dogs in houses] and [cats], cannot distinguish dog, house, cat; In other words, may have coordination ambiguity\n",
    "- <img src=\"./figure/pcfg2.png\" width=\"500\">\n",
    "- Usage in Language Modelling\n",
    "    - N-gram: can only model a couple words of context\n",
    "    - PCFG: capable of using whole context \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting keyphrases\n",
    "\n",
    "- The GRAMMAR is a regular expression used by the NLTK RegexpParser to create trees with the label KT (key term)\n",
    "\n",
    "    - `GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'`\n",
    "    - `chunker = RegexpParser(GRAMMAR)`  \n",
    "\n",
    "\n",
    "- \"The key terms and keyphrases contained within our corpora often provide insight into the topics or entities contained in the documents being analyzed.\"\n",
    "\n",
    "\n",
    "- Can be also used for feature extraction purpose to reduce dimensionality\n",
    "    - Input: document; Output: [key_phrase1, key_phrase2, ...]\n",
    "    - Uses PoS tags to identify adverbial phrases (“without care”) and adjective phrases (“terribly helpful”), and use keyphrases for sentiment analysis.\n",
    "\n",
    "## summary\n",
    "Disadvantage of grammer-based parsing/feature extraction\n",
    "- relies on a good tagger (PoS, n., v., adj., etc)\n",
    "- Non-standard, unseen structures\n",
    "- Have to define grammer at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frequency and tf-idf\n",
    "- Characterize a word based on words within a window-size\n",
    "<img src=\"./figure/word_bow.png\" width=\"600\">\n",
    "<img src=\"./figure/word_bow2.png\" width=\"600\">\n",
    "- Result: **Sparse** and **Long** vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Word Embedding \n",
    "\n",
    "Motivation:\n",
    "- Short and Dense\n",
    "- Capture Synonym\n",
    "- Less params to avoid overfitting\n",
    "\n",
    "\n",
    "### Word2vec\n",
    "\n",
    "- Why other Options not working\n",
    "    * One-hot vectors (vocabulary list too big; No similarity measurement; how about new words)\n",
    "    * Co-currence vector (matrix given a certain window size, # of times two words are together)->Sparsity\n",
    "    * Singular Vector Decomposition (SVD) for cocurrence matrix (too expensive)\n",
    "    * Use a word's context to represent --> Word embedding\n",
    "    \n",
    "    \n",
    "    \n",
    "- Key Components\n",
    "    * Center word *c*, context word *o*\n",
    "    * Two vectors for each word *w*: $ v_w $ and $ u_w $. $\\theta$ contains all *u* and *v* (Input and Output Vector)\n",
    "    * For example: $ P( w_2|w_1 ) = P(w_2|w_1;  u_{w2}, v_{w1}, \\theta )$\n",
    "    * Loss Function: $ J(\\theta) = -\\frac{1}{T}\\sum_{t}\\sum_{m\\in window} P(w_{t+j}|w_t)$\n",
    "    * Calculate u*v for each word, and use softmax to derive probability\n",
    "      - $ P(O|C) = \\frac{exp(u_o^T v_c)}{\\sum_{w}exp(u_w^T v_c)} $  \n",
    "      - $w$ is entire vocabulary\n",
    "        \n",
    "    * After optimization for loss, get two vectors for each word. Combine or Use *u* or Use *v*\n",
    "    * Can also be learned through a logistic regression\n",
    "    \n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=\"600\">    \n",
    "    \n",
    "- Variation\n",
    "    * Skip-grams (SG):given center, predict context\n",
    "    * Countinous Bag of Words (CBOW):given bag of context, predict center\n",
    "    * Negative sampling (maximize p of actual context + minimize p of random context i.e. noise)\n",
    "    * GloVe: combine count-based and direct-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Document Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word (BoW) Model\n",
    "<img src=\"./figure/BOW.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency encoding: long tail of more signoficant words\n",
    "- One-hot encoding: lose information of difference between words\n",
    "- TF-IDF: captures frequency; eliminate stop-words effect\n",
    "- TF-IDF: $w_{i,j} = tf_{i,j} \\times log(\\frac{N}{df_i})$, where \n",
    "    - term i and document j\n",
    "    - $tf_i$ is number of documents containing term i\n",
    "    - $df_{ij}$ is number of ocurrences of term i in document j\n",
    "    - N is total number of documents\n",
    "  \n",
    "  \n",
    "- ***High dimension when vocabulary increases***\n",
    "- ***Cannot deal with synonym***\n",
    "\n",
    "\n",
    "## Matrix Decomposition\n",
    "- LSA (Latent semantic analysis)\n",
    "    - Or: **non-negative matrix factorization (NNMF)**\n",
    "    - Perform SVD on TF-IDF matrix\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\" width=\"500\">\n",
    "   <img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0613.png\" width=\"500\">\n",
    "    - Compare 2 terms\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box3.png\" width=\"200\">\n",
    "    - Compare 2 documents\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box4.png\" width=\"300\">\n",
    "- Overall problem of vectorization with BoW models\n",
    "    - cannot measure similarity when sharing no terms\n",
    "    - high dimensionality with big sparseness\n",
    "    - lose information on grammar, semantic meanings\n",
    "    \n",
    "- ***Hard to explain the physical meaning of SVD***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detailed example and practice: see `Word_Embedding.ipynb`\n",
    "\n",
    "- Neural network serves as a look up table\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/flowchart-PerceptronLookup.png\" width=400>\n",
    "\n",
    "\n",
    "- Illustration of CBOW model: Ref: http://building-babylon.net/tag/word2vec/\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/context.png\" width=\"600\">\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/cbow.png\" width=\"600\">\n",
    "\n",
    "- Anotjer illustration of CBOW model\n",
    "<img src=\"https://i.stack.imgur.com/sAvR9.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector similarity\n",
    "    - Cosine similarity: <img src=\"https://qph.fs.quoracdn.net/main-qimg-fd48a47fdc134d6fc9b58cd86fdf244b\" width=300>\n",
    "    \n",
    "- ***LM model: representation layer; need to be combined with learning layer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed length with lower dimensionality for a document\n",
    "- A paragraph vector is added. The paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model.\n",
    "- Implementation in `gensim`: `model = Doc2Vec(corpus, size=5, min_count=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*9tVCGDm-ytPydhtJWVx3Zw.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed application, see another notebook `yelp_nlp.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0610.png\" width=\"500\">\n",
    "<img src=\"https://s3.amazonaws.com/skipgram-images/LDA.png\" width= \"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Modeling\n",
    "- Task Definition: \n",
    "    - Predict next word\n",
    "    - Assign probabilities to sequence of words\n",
    "- For example \n",
    "    - google search, spellig check, speed recognition, machine translation\n",
    "\n",
    "## N-gram model\n",
    "   - Prototype: $P(L = 3, e_1 = I, e_2 = am, e_3 = WS) = P(e_1 = I) \\times P(e_2 = am | e_1 = I) \\times P(e_3 = WS | e_1 = I, e_2 = am) \\times P(e_4 = EoS |e_1 = I, e_2 = am, e_3 = WS)$ \n",
    "       - NOTE: from beginning of sentence\n",
    "   - Advantage of N-gram: Using count of different length of grams as they shown in corpus\n",
    "   \n",
    "   \n",
    "   - For example: 2-gram (bigram). Calculate probs of \"I am\", \"am WS\", \"WS EoS\" instead of \"I am WS EoS\"\n",
    "       - $P(e_t|e^{t-1}_1) = P(e_t|e_{t-1}) $]\n",
    "       - $P(e_t|e_{t-1}) = \\frac{C(e_t, e_{t-1})}{C(e_{t-1})}$\n",
    "       - Can be generalized to 3-gram model\n",
    "       - <img src=\"./figure/2-gram.png\" width=\"400\">\n",
    "\n",
    "\n",
    "   - Main problem: **Sparsity**, some senetence may not appear in training set, joint probability will be zero (The same problem as Prototype)\n",
    "   - Fix by **smoothing**/**interpolation**: $P(e_t|e_{t-1}) = (1-\\alpha)P_{ML}(e_t|e_{t-1}) + \\alpha P_{ML}(e_t)$\n",
    "       - A combination of unigram and bigram to ensure P>0\n",
    "       - Variation: more grams, context-dependent alpha, etc\n",
    "   - Fix unknown words by adding a \"**unk**\" word\n",
    "       - Replace singletopns or words only appearing once/few times in training corpus with $unk$\n",
    "       - Pre-define a closed vocabulary list $V$, and convert words in training set and not in $V$ to be $unk$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NN Model with Fixed Window Size      \n",
    "   - No Sparsity problem (using input embedding M, so possible to traet similar words similarly during prediction)\n",
    "   - Model size reduced (Instead of learning all probs {a, b, c} X {A, B, C}, Neural network learn weights to represent the quadratic combination)\n",
    "   - Ability to skip a previous word\n",
    "   - BUT: X do not share weight, and how to decide window size\n",
    "\n",
    "\n",
    "<img src=\"./figure/nn_lm.png\" width=\"700\">\n",
    "<img src=\"./figure/nn_lm_learn_embed.png\" width=\"700\">\n",
    "\n",
    "### RNN Model\n",
    "   - Any sequence length will work\n",
    "   - Weights shared, model size doesn't increase\n",
    "   - BUT: computation is slow (why) and cannot access information from many steps back\n",
    "   - Others applications of RNN\n",
    "        * One-to-one: tagging each word\n",
    "        * many-to-one: sentiment analysis\n",
    "        * Encoder module: example: element-wise max of all hidden states -> as input for further NN model\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*q1wyldq3Nm5pT266eXdfzA.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation of LM\n",
    "- Given 1) Test dataset, and 2) trained language model P with parameter $\\theta$\n",
    "- Log likelihood $log(E_{test};\\theta) = \\sum_{E\\in E_{test}}{log[P(E;\\theta)]}$\n",
    "- Perplexity: $ ppl(E_{test};\\theta) = exp(-log(E_{test};\\theta) / len(E_{test}))$\n",
    "    - ppl = 1: perfect\n",
    "    - ppl = Vocabulary size: random model\n",
    "    - ppl = +inf: worst model\n",
    "    - ppl = some value $v$: need to pick $v$ values on average to get the correct one\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0502.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Word Window Classification\n",
    "- Difference with typical ML: learn both **W** and word vectors **x**\n",
    "- Task definition: classify a word in its *Context Window*\n",
    "    * Advantage: Do not train single word: ambiguity\n",
    "    * Advantage: Do not just average over window: lose position information\n",
    "    * Get a vector X with length of 5d where 5 is window size and d is embedding size\n",
    "    * Predict y based on softmax of WX and minimize cross-entropy error\n",
    "    \n",
    "    \n",
    "- Example: NER (*Named Entity Recognition*)\n",
    "    * 'Museums in Paris are good\". Binary task: whether Paris is a *location* or not.\n",
    "    \n",
    "    \n",
    "- What happens for word embedding x:\n",
    "    * Updated just as weigh W\n",
    "    * Pushed into an area helpful for classification task\n",
    "    * Example: $X_{in}$ may be a sign for location\n",
    "\n",
    "\n",
    "- Some **disadvantages** of window-based methods\n",
    "    - Anything outside the context window is ignored\n",
    "    - Hard to learn systematic pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.stack.imgur.com/a6CJc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "- Concepts\n",
    "    - Closed class: \n",
    "        - Prepositions: on, in\n",
    "        - Particles: (turn sth) over\n",
    "        - Determiner: a, an, the, this, that\n",
    "        - Conjunctions: and, or, but\n",
    "        - Pronouns: my, your, who\n",
    "    - Open classes:\n",
    "        - Nouns\n",
    "        - Verbs\n",
    "        - Adjectives\n",
    "        - Adverbs\n",
    "        \n",
    "- Tagset:\n",
    "    - 45-tag Penn Treebank tagset\n",
    "    \n",
    "    \n",
    "- Training set\n",
    "    - Corpora labeled with parts-of-speech\n",
    "    \n",
    "- Ambiguity:\n",
    "    - Example: book (noun. or verb.)\n",
    "    - Solution1: Most Frequent Class Baseline: for example: $a$ is a determiner instead of a letter in most cases\n",
    "    \n",
    "\n",
    "## Hidden-Markov-Model (HMM) for PoS tagging\n",
    "\n",
    "### Prepare transion matrix from training examples\n",
    "- A matrix: tag transition matrix\n",
    "    - VB: verb, MD: modal verb like \"will\"\n",
    "    - $P(VB|MD) = \\frac{C(MD, VB)}{C(MD)} = 0.8$\n",
    "- B matrix: emission matrix\n",
    "    - $P(\"will\"|MD) = \\frac{C(MD, \"will\")}{C(MD)} = 0.3$\n",
    "    \n",
    "<img src=\"./figure/Hmm.png\" width=\"600\"> \n",
    "### Define Optimal solution\n",
    "- Solve by NB\n",
    "- $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) \\\\\n",
    "\\xrightarrow{Bayesian} P(T_{1-n})P(W_{1-n}|T_{1-n}) \\\\\n",
    "\\xrightarrow{Inpendence\\ Assumption} P(T_{1-n}) \\prod_{i=1}^n P(W_i|T_i) \\\\\n",
    "\\xrightarrow{Bigram\\ Assumption}  \\prod_{i=1}^n P(T_i|T_{i-1}) \\prod_{i=1}^n P(W_i|T_i) \\\\\n",
    "=  \\prod_{i=1}^n P(T_i|T_{i-1})P(W_i|T_i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Algorithm\n",
    "- The Viterbi Algorithm\n",
    "    - Detailed TBA\n",
    "    - Essentially dynamic programming problem\n",
    "    \n",
    "- Beam search\n",
    "    - Better computational efficiency\n",
    "    \n",
    "<img src=\"./figure/bs.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions:\n",
    "- Modify Bigram Assumption to be Trigram Assumption: $\\prod_{i=1}^n P(T_i|T_{i-1}, T_{i-2})P(W_i|T_i)$\n",
    "- Add awareness of sentence end: $[\\prod_{i=1}^n P(T_i|T_{i-1}, T_{i-2})P(W_i|T_i)]\\ P(T_{n+1}|T_n)$\n",
    "- Data interpolations to fix sparseness: similar to smoothing in n-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Markov-Model (MEMM)\n",
    "<img src=\"./figure/hmm_memm.png\" width=\"600\">\n",
    "-  HMM/Generative: $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) \\\\\n",
    "\\xrightarrow{Bayesian} P(T_{1-n})P(W_{1-n}|T_{1-n}) =  \\prod_{i=1}^n P(T_i|T_{i-1})P(W_i|T_i)$ \n",
    "\n",
    "\n",
    "- MEMM/Discriminative: $Tag^*_{1-n} = {argmax}_t P(T_{1-n}|W_{1-n}) = \\prod_{i=1}^n P(T_i|T_{i-1}, W_i)$\n",
    "\n",
    "\n",
    "- Ability to contain feature sets besides simply $w_i$ and $t_{i-1}$.\n",
    "    - wi contains a particular prefix (from all prefixes of length ≤ 4)\n",
    "    - wi contains a particular suffix (from all suffixes of length ≤ 4)\n",
    "    - wi contains a number\n",
    "    - wi contains an upper-case letter\n",
    "    - wi contains a hyphen\n",
    "    - prefix\n",
    "    - suffix\n",
    "    - ......\n",
    "    \n",
    "- Solution Algorithm: Viterbi algorithm just as with the HMM\n",
    "\n",
    "\n",
    "### Bidirectionality\n",
    "- Conditional random field or CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "### Prepare\n",
    "- Evaluation Metrics\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - Accuracy\n",
    "    - F1\n",
    "    - AUC\n",
    "    \n",
    "    \n",
    "- Data collection:\n",
    "    - Web scrapping, crawl from internet\n",
    "    - Human annotation\n",
    "    - Data balancing\n",
    "\n",
    "\n",
    "- Prepare dictionary\n",
    "    - Emotion icons (human labels), replace with text\n",
    "    - Acronym dictionary (lol), replace with full form\n",
    "\n",
    "### Model \n",
    "\n",
    "- Baseline Model\n",
    "    - Lexicon-based method: Count +/- words\n",
    "        - Example from *MPQA lexicon*\n",
    "        - \\+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great\n",
    "        - \\− : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\n",
    "\n",
    "\n",
    "- ***Text Mining Features***\n",
    "    - Preprocessing\n",
    "        - Remove stopword\n",
    "        - Remove tags\n",
    "        - Remove urls\n",
    "        - Lemmatization\n",
    "        - Stemming\n",
    "        - Tokenization\n",
    "    - Unigram/Bigram (I go out, (I go, go out))\n",
    "        - Bigram addressed the problem of only considering single word, but larger vector\n",
    "        - Add negation:\n",
    "            - Example: did**n’t** like this movie , but I\n",
    "            - Becomes: didn’t **NOT_like** **NOT_this** **NOT_movie** , but I\n",
    "    - Bag of Word \n",
    "    - TF-IDF \n",
    "    - **Problem**: dimension too big\n",
    "\n",
    "\n",
    "- Naive Bayes  (Calculate $P(k), P(X|k)$)\n",
    "    - <img src=\"./figure/NB_al.png\" width=\"300\">\n",
    "    -  Naïve Bayes classifiers assume that the effect of a variable value on a given class is independent of the values of other variable. This assumption is called class conditional independence.\n",
    "    - ***Good for small dataset, and easy to train***\n",
    "    - Numerical Example:\n",
    "    <img src=\"./figure/NB_example.png\" width=\"400\">\n",
    "    - P(+) =, P(-) = \n",
    "    - P(predictable|+) = ...\n",
    "    - P(predictable|-) = ...\n",
    "    - P(+)P(S|+) = ...\n",
    "    - P(-)P(S|-) = ...\n",
    "    \n",
    "    - Some improvements for sentiment analysis:\n",
    "        - Code word appearance instead of frequency\n",
    "\n",
    "\n",
    "\n",
    "- ***Environment Features***\n",
    "    - Location\n",
    "    - Time\n",
    "    - Device\n",
    "    - .....\n",
    "\n",
    "\n",
    "- ***Linguistic Features***\n",
    "    - **Human efforts involved**\n",
    "    - Length of comment\n",
    "    - Number of negative words\n",
    "    - Sum of prior scores\n",
    "    - Number of hashtags\n",
    "    - Number of handles\n",
    "    - Repeat characters\n",
    "    - Exclamation, Capitalized tesxt\n",
    "    - Number of new lines\n",
    "        \n",
    "<img src=\"./figure/logistic_feature.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- Comparison between NB and LR\n",
    "    - NB: Strong conditional independence assumption\n",
    "    - LR: better estimate correlated features\n",
    "    - LR: add regularizations:\n",
    "        - L1: Laplace Prior\n",
    "        - L2: Gaussian Distribution with zero meam\n",
    "        - Bayesian interpretations: ${argmax}_w [P(Y|w,x)P(w)] = {argmax}_w [Log(P(Y|w,x)) - \\alpha \\sum w^2]$ when $P(w)$ is zero mean Gaussian\n",
    "\n",
    "\n",
    "\n",
    "- Machine Learning Algorithm\n",
    "    - SVM\n",
    "    - DT/RF/BT\n",
    "    \n",
    "\n",
    "- Deep Learning Algorithm\n",
    "    - Set sequence length\n",
    "    - Set vocabulary size\n",
    "    - Word embedding\n",
    "        - Pre-trained (GloVe)\n",
    "        - Online-traning\n",
    "    - CNN\n",
    "    - LSTM\n",
    "        - E.g., word \"not\" can be rotating the polarity of the next word\n",
    "      \n",
    "      \n",
    "- Syntactic meanings\n",
    "    - Dependency parsing\n",
    "    - Coreference\n",
    "    - Sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguity\n",
    "https://slideplayer.com/slide/5187871/\n",
    "\n",
    "Background Example:\n",
    "- **Lemma**: Mouse, has 2 **senses**\n",
    "    1. any of numerous small rodents...\n",
    "    2. a hand-operated device that controls a cursor...\n",
    "\n",
    "\n",
    "- Words with the same **sense**: ***Synonyms***\n",
    "- Words with the opposite **senses**: ***Antonym***\n",
    "- Words with similarity: similar words (cat, dog)\n",
    "- Words with relatedness: (coffee, cup, waiter, menu, plate,food, chef), they belong to the same **senamtic field**\n",
    "- Words with taxonomic relations: we say that vehicle is a **hypernym** of car, and dog is a **hyponym** of animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "- Just pick most frequent sense\n",
    "\n",
    "\n",
    "### Knowledge-Based\n",
    "\n",
    "***How to define similarity***\n",
    "- Each word has gloss, example, etc.\n",
    "- Lesk Algorithm: $Score\\ (sense_i, context_j) = similarity\\ (example_i, context_j)$. For example: bank vs. deposits\n",
    "- Similarity can be defined by, e.g., percent of overlapping words\n",
    "\n",
    "\n",
    "***Pro/Con***\n",
    "- One model for all\n",
    "- Can use similar words / synosym if example is limited\n",
    "- These algorithms are overlap based, so they suffer from overlap sparsity and performance depends on dictionary definitions.\n",
    "\n",
    "\n",
    "### Supervised \n",
    "\n",
    "***How to featurize a sample text***\n",
    "- Collocational features: Position-specific information\n",
    "    - *\"guitar and --bass-- player stand\"*\n",
    "    - Feature: POS tag for targets and neighbors, and context words: $[w_{i-2}, POS_{i-2}...,w_{i+2}, POS_{i+2} ]$\n",
    "\n",
    "\n",
    "- Other syntactic features of the sentence\n",
    "    - Passive or not\n",
    "\n",
    "\n",
    "- BoW features\n",
    "    - Vocabulary list: [[fishing,\tbig,\tsound,\tplayer,\tfly,\trod,\tpound,\tdouble,\truns,\tplaying,\tguitar,\tband]\t]\n",
    "    - Feature: [0,0,0,1,0,0,0,0,0,0,1,0]\t\n",
    "    \n",
    "***Apply classfication algorithms***\n",
    "- NB\n",
    "- SVM\n",
    "\n",
    "\n",
    "***Pro/Con***\n",
    "- This type of algorithms are better than the two approaches w.r.t.implementation perspective.\n",
    "- These algorithms don’t give satisfactory result for resource scarce languages. \n",
    "- Need to train it for each word\n",
    "\n",
    "### Semi-supervised\n",
    "- Bootstrapping\n",
    "\n",
    "### Unsupervised\n",
    "- Word-sense induction\n",
    "- Topic modelling\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0601.png\" width= \"500\">\n",
    "\n",
    "- Some key points\n",
    "    - Definition of distance: cosine distance, Euclidean distance\n",
    "    - Vectorization: one-hot, TF-IDF\n",
    "    - Algorithm: K-means, Agglomerative clustering\n",
    "    - Number of clusters $K$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Matching\n",
    "### Deep Structured Semantic Models\n",
    "<img src=\"https://raw.githubusercontent.com/v-liaha/v-liaha.github.io/master/assets/dssm.png\" width=\"800\">\n",
    "​\n",
    "To Read:\n",
    "https://cloud.tencent.com/developer/article/1173704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "## Problem definition\n",
    "- Neural Machine Translation (NMT)\n",
    "- Sequence-to-Sequence(seq2seq) architecture\n",
    "- Difference from SMT (Statistical MT): calculate P(y|x) directly instead of using Bayes\n",
    "- Advantage: Single NN, less human engineering\n",
    "- Disadvantage: less interpretable, less control\n",
    "\n",
    "## Main Components\n",
    "- Encoder RNN: encode source sentence, generate hidden state\n",
    "- Decoder RNN: **Language Model**, generate target sentence using outputs from encoder RNN; predict next word in *y* conditional on input *x*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1585/1*sO-SP58T4brE9EHazHSeGA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allow information from future inputs\n",
    "* LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google’s neural machine translation (Google-NMT) \n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0109.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "\n",
    "- Greedy decoding problem\n",
    "    * Instead of generating argmax each step, use beam search.\n",
    "    * Keep *k* most probable translations\n",
    "    * Exactly *k* nodes at each time step *t*\n",
    "    * *Note*: Length bias, prefer shorter sentence because the log(P) accumulates. Can add prior for sentence length to compensate.\n",
    "    \n",
    "<img src=\"./figure/beam.png\" width=\"300\">\n",
    "https://arxiv.org/pdf/1703.01619.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention model\n",
    "\n",
    "### General Advantage\n",
    "\n",
    "- Focus on certain parts of source (Instead of encoding whole sentence in **one** hidden vector.)\n",
    "- Provides shortcut / Bypass bottleneck\n",
    "- Get some interpretable results and learn alignment\n",
    "- \"Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM\"\n",
    "\n",
    "<img src=\"https://pic1.zhimg.com/80/v2-d266bf48a1d77e7e4db607978574c9fc_hd.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong attention mechanism\n",
    "\n",
    "\n",
    "\n",
    "1. Get ***encoder*** hidden states: $ h_1, ..h_k,..., h_N $\n",
    "\n",
    "1. Get ***decoder*** hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1})$<br/><br/>\n",
    "    \n",
    "1. Get attention scores by dot product: \n",
    "$ \\mathbf e_t = [s^T_t h_1, ..., s^T_t h_N] $\n",
    "    - Other alignment options available <br/>\n",
    "    <img src=\"https://i.stack.imgur.com/tiQkz.png\" width=\"300\"> \n",
    "    - Penalty available: penalize input tokens that have obtained high attention scores in past decoding steps \n",
    "    - $e'_t = e_t\\ if\\ t = 1\\ else\\ \\frac{exp(e_t)}{\\sum_{j=1}^{t-1}{exp(e_j)}} $ for decoder state\n",
    "    \n",
    "    \n",
    "4. Take softmax of $ \\mathbf e_t $ and get $ \\pmb\\alpha_t $ which sum up to one\n",
    "    - $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - Note: $\\pmb\\alpha_t$ can be interpreted as attention. For example, when generating word `vas`, the attention for `are` in encoder hidden states should be close to 1, others to 0<br/><br/>\n",
    "    \n",
    "    \n",
    "5. Take weighted sum of hidder states $\\mathbf h$ and $\\pmb\\alpha$, and get context vector **c**\n",
    "    - $ c_t = \\sum_{k=1}^{N} \\alpha_{tk}h_k $<br/><br/>\n",
    "    \n",
    "6. Generate *Attentional Hidden Layer*\n",
    "    - $ \\tilde h_t = tanh(W_c[c_t;s_t])$<br/><br/>\n",
    "\n",
    "7. Make Predicition\n",
    "    - $ p = softmax(W_s \\tilde h_t)$\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Depiction-of-Global-Attention-in-an-Encoder-Decoder-Recurrent-Neural-Network.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Mechanism\n",
    "\n",
    "**Main difference**\n",
    "\n",
    "1. Get attention scores by dot product: \n",
    "    - $ \\mathbf e_t = [s^T_{t-1} h_1, ..., s^T_{t-1} h_N] $<br/><br/>\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1}, c_t)$<br/><br/>\n",
    "    \n",
    "1. Make Predicition: \n",
    "    - $ p = softmax(g(s_t))$\n",
    "\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of two mechanism**\n",
    "\n",
    "<img src=\"http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png\" width=\"800\">\n",
    "\n",
    "**Example of attention weights**\n",
    "<img src=\"https://i.stack.imgur.com/WxG8e.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "- RNN (LSTM): difficult to predict rare or out-of-vocabulary words\n",
    "- Pointer Network: generate word from input sentence (i.e., OoV - out of Vocabulary words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/efbd381493bb9636f489b965a2034d529cd56bcd/1-Figure1-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part I: Seq2Seq Attention Model\n",
    "    - See above\n",
    "    - $p_{vocabulary}(word)$\n",
    "    \n",
    "    \n",
    "- Part II: Pointer Generator\n",
    "    - After getting $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - $p_{pointer}(word) = \\sum \\alpha_t$, where position t is actually word w\n",
    "\n",
    "\n",
    "- Weighted sum: \n",
    "    - $g * p_{vocabulary}(word) + (1-g) * p_{pointer}(word) $\n",
    "    \n",
    "    \n",
    "- Applications:\n",
    "    - Summarization\n",
    "    - Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self/Intra/Inner Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute alignment function f among **decoder** hidden states $s_t$\n",
    "- Apply softmax for all states before current time $t$\n",
    "- Weighted sum will get current decoder attention output $c^d_t$\n",
    "- Why self-attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog Systems and Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution\n",
    "## Coreference and Anaphora\n",
    "- Barak Obama travelled to,..., Obama\n",
    "- Obama says that he ....\n",
    "\n",
    "## Mention detection\n",
    "- Pronouns (I, your, it) - Part-of-Speech (POS) tagger\n",
    "- Named Entity (People name, place. tec) - NER system\n",
    "- Noun phrase (The dog stuck in the hole) - constituency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreferece model\n",
    "- Mention pair\n",
    "    * For each word, look at candidate antecedents, and train a **binary** classifier to predict $p(m_i,m_j)$\n",
    "\n",
    "- Mention rank\n",
    "    * Apply softmax to all candidate antecedents, and add highest scoring coreference link\n",
    "    * Each mention is only linked to **one** antecedent\n",
    "    \n",
    "\n",
    "- Clustering\n",
    "\n",
    "\n",
    "- Neural Coref Model\n",
    "    * Input layer: word embedding and other catogorical features (e.g., distance, document characteristic)\n",
    "<img src=\"./figure/Coref.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- End-to-end Model\n",
    "    * No separate mention detection step\n",
    "    * Apply LSTM and attention\n",
    "    * Consider span of text as a candidiate mention\n",
    "    * Final score: $s(i, j) = s_m(i) + s_m(j) + s_a(i, j)$, which means Is i, j mentions, and do they look coreferent.\n",
    "    \n",
    "<img src=\"./figure/endtoend.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/07/comparison-top-6-python-nlp-libraries.html\n",
    "<img src=\"https://activewizards.com/content/blog/Comparison_of_Python_NLP_libraries/nlp-librares-python-prs-and-cons01.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Answering System: https://mp.weixin.qq.com/s/Jq86VtlX29u9xuLE2E4wKQ"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "469px",
    "left": "0px",
    "right": "896.646px",
    "top": "107px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
