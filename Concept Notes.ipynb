{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collaborative filtering\n",
    "text clustering\n",
    "NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic concepts\n",
    "## terminology\n",
    "- semantic: relating to meaning in language or logic\n",
    "- syntactic: of or according to syntax\n",
    "\n",
    "## Overall flow\n",
    "1. Checks the location on disk to make sure no errors occur.\n",
    "2. Gets all paragraphs for the given text.\n",
    "3. Segements the paragraphs with the sent_tokenizer\n",
    "4. Tokenizes the sentences with the wordpunct_tokenizer\n",
    "5. Tags the sentences using the default pos_tagger\n",
    "6. Writes the document as a pickle to the target location.\n",
    "\n",
    "## sentence segmentation\n",
    "- Input: paragraph\n",
    "- Output: sentences\n",
    "\n",
    "## word tokenization / text normalization\n",
    "- Input: sentence\n",
    "- Output: tokens\n",
    "    - contraction: is not --> isn't\n",
    "    - punctuation marks\n",
    "    \n",
    "## part-of-Speech Tagging\n",
    "- Input: sentence with tokens\n",
    "- Output: tuples of (tag, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic parsing\n",
    "\n",
    "## context free grammar\n",
    "\n",
    "`GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "    \"\"\"`\n",
    "\n",
    "## extracting keyphrases\n",
    "\n",
    "- The GRAMMAR is a regular expression used by the NLTK RegexpParser to create trees with the label KT (key term)\n",
    "\n",
    "    - `GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'`\n",
    "    - `chunker = RegexpParser(GRAMMAR)`  \n",
    "\n",
    "\n",
    "- \"The key terms and keyphrases contained within our corpora often provide insight into the topics or entities contained in the documents being analyzed.\"\n",
    "\n",
    "\n",
    "- Can be also used for feature extraction purpose to reduce dimensionality\n",
    "    - Input: document; Output: [key_phrase1, key_phrase2, ...]\n",
    "    - Uses PoS tags to identify adverbial phrases (“without care”) and adjective phrases (“terribly helpful”), and use keyphrases for sentiment analysis.\n",
    "\n",
    "\n",
    "## named-entity\n",
    "- relies on PoS tagging; details TBA\n",
    "\n",
    "## summary\n",
    "Disadvantage of grammer-based parsing/feature extraction\n",
    "- relies on a good tagger (PoS, n., v., adj., etc)\n",
    "- Non-standard, unseen structures\n",
    "- Have to define grammer at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Document Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency encoding: long tail of more signoficant words\n",
    "- One-hot encoding: lose information of difference between words\n",
    "- TF-IDF: captures frequency; eliminate stop-words effect\n",
    "- TF-IDF: $w_{i,j} = tf_{i,j} \\times log(\\frac{N}{df_i})$, where \n",
    "    - term i and document j\n",
    "    - $tf_i$ is number of documents containing term i\n",
    "    - $df_{ij}$ is number of ocurrences of term i in document j\n",
    "    - N is total number of documents\n",
    "- LSA (Latent semantic analysis)\n",
    "    - Or: **non-negative matrix factorization (NNMF)**\n",
    "    - Perform SVD on TF-IDF matrix\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\" width=\"500\">\n",
    "   <img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0613.png\" width=\"500\">\n",
    "    - Compare 2 terms\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box3.png\" width=\"200\">\n",
    "    - Compare 2 documents\n",
    "    <img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box4.png\" width=\"300\">\n",
    "- Overall problem of vectorization with BoW models\n",
    "    - cannot measure similarity when sharing no terms\n",
    "    - high dimensionality with big sparseness\n",
    "    - lose information on grammar, semantic meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Word Embedding \n",
    "### Word2vec\n",
    "\n",
    "- Why other Options not working\n",
    "    * One-hot vectors (vocabulary list too big; No similarity measurement; how about new words)\n",
    "    * Co-currence vector (matrix given a certain window size, # of times two words are together)->Sparsity\n",
    "    * Singular Vector Decomposition (SVD) for cocurrence matrix (too expensive)\n",
    "    * Use a word's context to represent --> Word embedding\n",
    "    \n",
    "    \n",
    "    \n",
    "- Key Components\n",
    "    * Center word *c*, context word *o*\n",
    "    * Two vectors for each word *w*: $ v_w $ and $ u_w $. $\\theta$ contains all *u* and *v* (Input and Output Vector)\n",
    "    * For example: $ P( w_2|w_1 ) = P(w_2|w_1;  u_{w2}, v_{w1}, \\theta )$\n",
    "    * Loss Function: $ J(\\theta) = -\\frac{1}{T}\\sum_{t}\\sum_{m\\in window} P(w_{t+j}|w_t)$\n",
    "    * Calculate u*v for each word, and use softmax to derive probability\n",
    "      - $ P(O|C) = \\frac{exp(u_o^T v_c)}{\\sum_{w}exp(u_w^T v_c)} $  \n",
    "      - $w$ is entire vocabulary\n",
    "        \n",
    "    * After optimization for loss, get two vectors for each word. Combine or Use *u* or Use *v*\n",
    "    \n",
    "    \n",
    "    \n",
    "- Variation\n",
    "    * Skip-grams (SG):given center, predict context\n",
    "    * Countinous Bag of Words (CBOW):given bag of context, predict center\n",
    "    * Negative sampling (maximize p of actual context + minimize p of random context i.e. noise)\n",
    "    * GloVe: combine count-based and direct-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed length with lower dimensionality for a document\n",
    "- A paragraph vector is added. The paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model.\n",
    "- Implementation in `gensim`: `model = Doc2Vec(corpus, size=5, min_count=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*9tVCGDm-ytPydhtJWVx3Zw.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detailed example and practice: see `Word_Embedding.ipynb`\n",
    "\n",
    "\n",
    "- Illustration of CBOW model: Ref: http://building-babylon.net/tag/word2vec/\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/context.png\" width=\"600\">\n",
    "\n",
    "<img src=\"http://building-babylon.net/wp-content/uploads/2017/07/cbow.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Modeling\n",
    "- Task Definition: Predict next word; \n",
    "- For example: google search\n",
    "\n",
    "## N-gram model\n",
    "   - Prototype: $P(L = 3, e_1 = I, e_2 = am, e_3 = WS) = P(e_1 = I) \\times P(e_2 = am | e_1 = I) \\times P(e_3 = WS | e_1 = I, e_2 = am) \\times P(e_4 = EoS |e_1 = I, e_2 = am, e_3 = WS)$ \n",
    "       - NOTE: from beginning of sentence\n",
    "   - Advantage of N-gram: Using count of different length of grams as they shown in corpus\n",
    "   - For example: 2-gram (bigram). Calculate probs of \"I am\", \"am WS\", \"WS EoS\" instead of \"I am WS EoS\"\n",
    "   - Main problem: **Sparsity**, some senetence may not appear in training set, joint probability will be zero (The same problem as Prototype)\n",
    "   - Fix by **smoothing**/**interpolation**: $P(e_t|e_{t-1}) = (1-\\alpha)P_{ML}(e_t|e_{t-1}) + \\alpha P_{ML}(e_t)$\n",
    "       - A combination of unigram and bigram to ensure P>0\n",
    "       - Variation: more grams, context-dependent alpha, etc\n",
    "   - Fix unknown words by adding a \"**unk**\" word\n",
    "       - Remove singletopns or words only appearing once in training corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NN Model with Fixed Window Size      \n",
    "   - No Sparsity problem (using input embedding M, so possible to traet similar words similarly during prediction)\n",
    "   - Model size reduced (Instead of learning all probs {a, b, c} X {A, B, C}, Neural network learn weights to represent the quadratic combination)\n",
    "   - Ability to skip a previous word\n",
    "   - BUT: X do not share weight, and how to decide window size\n",
    "    \n",
    "    \n",
    "<img src=\"http://kiyukuta.github.io/_images/nnlm_bengio.png\" width=\"500\">\n",
    "\n",
    "### RNN Model\n",
    "   - Any sequence length will work\n",
    "   - Weights shared, model size doesn't increase\n",
    "   - BUT: computation is slow (why) and cannot access information from many steps back\n",
    "   - Others applications of RNN\n",
    "        * One-to-one: tagging each word\n",
    "        * many-to-one: sentiment analysis\n",
    "        * Encoder module: example: element-wise max of all hidden states -> as input for further NN model\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*q1wyldq3Nm5pT266eXdfzA.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation of LM\n",
    "- Given 1) Test dataset, and 2) trained language model P with parameter $\\theta$\n",
    "- Log likelihood $log(E_{test};\\theta) = \\sum_{E\\in E_{test}}{log[P(E;\\theta)]}$\n",
    "- Perplexity: $ ppl(E_{test};\\theta) = exp(-log(E_{test};\\theta) / len(E_{test}))$\n",
    "    - ppl = 1: perfect\n",
    "    - ppl = Vocabulary size: random model\n",
    "    - ppl = +inf: worst model\n",
    "    - ppl = some value $v$: need to pick $v$ values on average to get the correct one\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0502.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Word Window Classification\n",
    "- Difference with typical ML: learn both **W** and word vectors **x**\n",
    "- Task definition: classify a word in its *Context Window*\n",
    "    * Advantage: Do not train single word: ambiguity\n",
    "    * Advantage: Do not just average over window: lose position information\n",
    "    * Get a vector X with length of 5d where 5 is window size and d is embedding size\n",
    "    * Predict y based on softmax of WX and minimize cross-entropy error\n",
    "    \n",
    "    \n",
    "- Example: NER (*Named Entity Recognition*)\n",
    "    * 'Museums in Paris are good\". Binary task: whether Paris is a *location* or not.\n",
    "    \n",
    "    \n",
    "- What happens for word embedding x:\n",
    "    * Updated just as weigh W\n",
    "    * Pushed into an area helpful for classification task\n",
    "    * Example: $X_{in}$ may be a sign for location\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "## Example of a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.stack.imgur.com/a6CJc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bicepjai/Deep-Survey-Text-Classification/master/images/paper_02_cnn_sent_model.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0601.png\" width= 500>\n",
    "\n",
    "- Some key points\n",
    "    - Definition of distance: cosine distance, Euclidean distance\n",
    "    - Vectorization: one-hot, TF-IDF\n",
    "    - Algorithm: K-means, Agglomerative clustering\n",
    "    - Number of clusters $K$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed application, see another notebook `yelp_nlp.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/applied-text-analysis/9781491963036/assets/atap_0610.png\" width=\"500\">\n",
    "<img src=\"https://s3.amazonaws.com/skipgram-images/LDA.png\" width= 500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation\n",
    "\n",
    "## Problem definition\n",
    "- Neural Machine Translation (NMT)\n",
    "- Sequence-to-Sequence(seq2seq) architecture\n",
    "- Difference from SMT (Statistical MT): calculate P(y|x) directly instead of using Bayes\n",
    "- Advantage: Single NN, less human engineering\n",
    "- Disadvantage: less interpretable, less control\n",
    "- Figure (TBA)\n",
    "\n",
    "## Main Components\n",
    "- Encoder RNN: encode source sentence, generate hidden state\n",
    "- Decoder RNN: **Language Model**, generate target sentence using outputs from encoder RNN; predict next word in *y* conditional on input *x*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1585/1*sO-SP58T4brE9EHazHSeGA.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allow information from future inputs\n",
    "* LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google’s neural machine translation (Google-NMT) \n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0109.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "\n",
    "- Greedy decoding problem\n",
    "    * Instead of generating argmax each step, use beam search.\n",
    "    * Keep *k* most probable translations\n",
    "    * Exactly *k* nodes at each time step *t*\n",
    "    * *Note*: Length bias, prefer shorter sentence because the log(P) accumulates. Can add prior for sentence length to compensate.\n",
    "    \n",
    "<img src=\"./figure/beam.png\" width=\"300\">\n",
    "https://arxiv.org/pdf/1703.01619.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention model\n",
    "\n",
    "### General Advantage\n",
    "\n",
    "- Focus on certain parts of source (Instead of encoding whole sentence in **one** hidden vector.)\n",
    "- Provides shortcut / Bypass bottleneck\n",
    "- Get some interpretable results and learn alignment\n",
    "- \"Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong attention mechanism\n",
    "\n",
    "\n",
    "\n",
    "1. Get encoder hidden states: $ h_1, ..h_k,..., h_N $\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1})$<br/><br/>\n",
    "    \n",
    "1. Get attention scores by dot product: \n",
    "$ \\mathbf e_t = [s^T_t h_1, ..., s^T_t h_N] $\n",
    "    - Other alignment options available <br/>\n",
    "    <img src=\"https://i.stack.imgur.com/tiQkz.png\" width=\"300\"> \n",
    "    - Penalty available: penalize input tokens that have obtained high attention scores in past decoding steps \n",
    "    - $e'_t = e_t\\ if\\ t = 1\\ else\\ \\frac{exp(e_t)}{\\sum_{j=1}^{t-1}{exp(e_j)}} $ for decoder state\n",
    "    \n",
    "    \n",
    "4. Take softmax of $ \\mathbf e_t $ and get $ \\pmb\\alpha_t $ which sum up to one\n",
    "    - $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - Note: $\\pmb\\alpha_t$ can be interpreted as attention. For example, when generating word `vas`, the attention for `are` in encoder hidden states should be close to 1, others to 0<br/><br/>\n",
    "    \n",
    "    \n",
    "5. Take weighted sum of hidder states $\\mathbf h$ and $\\pmb\\alpha$, and get context vector **c**\n",
    "    - $ c_t = \\sum_{k=1}^{N} \\alpha_{tk}h_k $<br/><br/>\n",
    "    \n",
    "6. Generate *Attentional Hidden Layer*\n",
    "    - $ \\tilde h_t = tanh(W_c[c_t;h_t])$<br/><br/>\n",
    "\n",
    "7. Make Predicition\n",
    "    - $ p = softmax(W_s \\tilde h_t)$\n",
    "\n",
    "\n",
    "<img src=\"./figure/attention.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Mechanism\n",
    "\n",
    "**Main difference**\n",
    "\n",
    "1. Get attention scores by dot product: \n",
    "    - $ \\mathbf e_t = [s^T_{t-1} h_1, ..., s^T_{t-1} h_N] $<br/><br/>\n",
    "\n",
    "1. Get decoder hidden state at time *t*: $ s_t $\n",
    "    - $s_t = LSTM(s_{t-1}, \\hat y_{t-1}, c_t)$<br/><br/>\n",
    "    \n",
    "1. Make Predicition: \n",
    "    - $ p = softmax(g(s_t))$\n",
    "\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of two mechanism**\n",
    "\n",
    "<img src=\"http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network\n",
    "- RNN (LSTM): difficult to predict rare or out-of-vocabulary words\n",
    "- Pointer Network: generate word from input sentence (i.e., OoV - out of Vocabulary words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/efbd381493bb9636f489b965a2034d529cd56bcd/1-Figure1-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part I: Seq2Seq Attention Model\n",
    "    - See above\n",
    "    - $p_{vocabulary}(word)$\n",
    "    \n",
    "    \n",
    "- Part II: Pointer Generator\n",
    "    - After getting $ \\pmb\\alpha_t = softmax(\\mathbf e_t) $\n",
    "    - $p_{pointer}(word) = \\sum \\alpha_t$, where position t is actually word w\n",
    "\n",
    "\n",
    "- Weighted sum: \n",
    "    - $g * p_{vocabulary}(word) + (1-g) * p_{pointer}(word) $\n",
    "    \n",
    "    \n",
    "- Applications:\n",
    "    - Summarization\n",
    "    - Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self/Intra/Inner Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute alignment function f among **decoder** hidden states $s_t$\n",
    "- Apply softmax for all states before current time $t$\n",
    "- Weighted sum will get current decoder attention output $c^d_t$\n",
    "- Why self-attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution\n",
    "## Coreference and Anaphora\n",
    "- Barak Obama travelled to,..., Obama\n",
    "- Obama says that he ....\n",
    "\n",
    "## Mention detection\n",
    "- Pronouns (I, your, it) - Part-of-Speech (POS) tagger\n",
    "- Named Entity (People name, place. tec) - NER system\n",
    "- Noun phrase (The dog stuck in the hole) - constituency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreferece model\n",
    "- Mention pair\n",
    "    * For each word, look at candidate antecedents, and train a **binary** classifier to predict $p(m_i,m_j)$\n",
    "\n",
    "- Mention rank\n",
    "    * Apply softmax to all candidate antecedents, and add highest scoring coreference link\n",
    "    * Each mention is only linked to **one** antecedent\n",
    "    \n",
    "\n",
    "- Clustering\n",
    "\n",
    "\n",
    "- Neural Coref Model\n",
    "    * Input layer: word embedding and other catogorical features (e.g., distance, document characteristic)\n",
    "<img src=\"./figure/Coref.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- End-to-end Model\n",
    "    * No separate mention detection step\n",
    "    * Apply LSTM and attention\n",
    "    * Consider span of text as a candidiate mention\n",
    "    * Final score: $s(i, j) = s_m(i) + s_m(j) + s_a(i, j)$, which means Is i, j mentions, and do they look coreferent.\n",
    "    \n",
    "<img src=\"./figure/endtoend.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/07/comparison-top-6-python-nlp-libraries.html\n",
    "<img src=\"https://activewizards.com/content/blog/Comparison_of_Python_NLP_libraries/nlp-librares-python-prs-and-cons01.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
