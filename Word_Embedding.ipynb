{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a `Skip-Gram` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data as list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def download(filename):\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename,local_filename)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = download('text8.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data into Vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "vocabulary = read_data(filename)\n",
    "print('Vocabulary size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `Count`, `Dictionary`, `Data`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \n",
    "    #### count --> [['UNK',?], (word1, count1), ....]\n",
    "    \n",
    "    count = [['UNK', -1]] #replace rare words with UNK\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1)) \n",
    "    \n",
    "    #### dictionary --> {'word1': index1}\n",
    "    \n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    # data --> [index1, index2, ...] \n",
    "    \n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # 'UNK'\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    \n",
    "    #### count --> [['UNK',count_0], (word1, count1), ....]\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    #### reversed_dictionary --> [{index1: word1}]\n",
    "    \n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5234, 3081, 12]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', 418391], ('the', 1061396), ('of', 593677)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: \n",
      "[3081 3081   12   12    6    6  195  195]\n",
      "Labels: \n",
      "[[  12]\n",
      " [5234]\n",
      " [3081]\n",
      " [   6]\n",
      " [  12]\n",
      " [ 195]\n",
      " [   6]\n",
      " [   2]]\n"
     ]
    }
   ],
   "source": [
    "print('Batch: ')\n",
    "print(batch)\n",
    "print('Labels: ')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format: index-middle, word-middle, index-left, word-left\n",
      "\n",
      "3081 originated -> 12 as\n",
      "3081 originated -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n"
     ]
    }
   ],
   "source": [
    "print('Format: index-middle, word-middle, index-left, word-left\\n')\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', \n",
    "          labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that every word has 2 neighboring words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 94, 37, 76, 71, 92, 32, 75, 87, 68, 61, 21, 53,  2, 23, 85])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph - 1, Embedding Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight matrix between -1 and 1, size is v_size * embedding size\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(100, 128) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "embed # Note, shape = (batch_size, embedding_size), i.e., word vec for all inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph - 2, From hidden layer to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss = tf.reduce_mean( #mean over batch\n",
    "      tf.nn.nce_loss(weights = nce_weights,\n",
    "                     biases = nce_biases,\n",
    "                     labels = train_labels,\n",
    "                     inputs = embed,\n",
    "                     num_sampled = num_sampled,\n",
    "                     num_classes = vocabulary_size))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph - 3, Add validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset) # find embedding vec for all valid in\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True) # calculate distance w/ all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_steps = 50001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  252.850860596\n",
      "Average loss at step  2000 :  112.89615527\n",
      "Average loss at step  4000 :  54.0413853676\n",
      "Average loss at step  6000 :  33.5787463242\n",
      "Average loss at step  8000 :  23.2382255834\n",
      "Average loss at step  10000 :  17.5596958621\n",
      "Nearest to is: was, and, fayetteville,\n",
      "Nearest to state: awards, generator, codes,\n",
      "Nearest to also: mishnayot, stake, material,\n",
      "Nearest to no: sourceforge, tracking, and,\n",
      "Nearest to world: pyruvate, plasma, loss,\n",
      "Nearest to system: lived, dryness, shakespeare,\n",
      "Nearest to be: deg, hailstones, pron,\n",
      "Nearest to d: repetitive, bay, jerseys,\n",
      "Nearest to known: fastened, rivers, precise,\n",
      "Nearest to see: annoyed, afternoons, how,\n",
      "Nearest to after: and, of, libation,\n",
      "Nearest to four: nine, zero, eight,\n",
      "Nearest to can: sex, shooter, vera,\n",
      "Nearest to of: and, in, for,\n",
      "Nearest to seven: nine, zero, aquila,\n",
      "Nearest to during: in, aquila, acids,\n",
      "Average loss at step  12000 :  14.3223680969\n",
      "Average loss at step  14000 :  11.7460552073\n",
      "Average loss at step  16000 :  10.3161996474\n",
      "Average loss at step  18000 :  8.83978385401\n",
      "Average loss at step  20000 :  8.31231600451\n",
      "Nearest to is: was, are, as,\n",
      "Nearest to state: awards, codes, trinomial,\n",
      "Nearest to also: bpp, chromosomal, UNK,\n",
      "Nearest to no: sourceforge, antipsychotics, nutation,\n",
      "Nearest to world: loss, precious, building,\n",
      "Nearest to system: lived, dryness, kashmir,\n",
      "Nearest to be: have, is, was,\n",
      "Nearest to d: b, repetitive, primigenius,\n",
      "Nearest to known: fastened, used, fansite,\n",
      "Nearest to see: annoyed, afternoons, and,\n",
      "Nearest to after: of, trinomial, enhancement,\n",
      "Nearest to four: eight, nine, six,\n",
      "Nearest to can: may, akita, sex,\n",
      "Nearest to of: in, and, for,\n",
      "Nearest to seven: eight, nine, five,\n",
      "Nearest to during: in, imprisonment, by,\n",
      "Average loss at step  22000 :  7.55049661839\n",
      "Average loss at step  24000 :  6.91815960693\n",
      "Average loss at step  26000 :  6.83181628847\n",
      "Average loss at step  28000 :  6.60855450153\n",
      "Average loss at step  30000 :  5.79118127155\n",
      "Nearest to is: was, are, has,\n",
      "Nearest to state: awards, trinomial, codes,\n",
      "Nearest to also: bpp, abraxas, chromosomal,\n",
      "Nearest to no: sourceforge, antipsychotics, recitative,\n",
      "Nearest to world: vma, sdp, loss,\n",
      "Nearest to system: dryness, lived, kashmir,\n",
      "Nearest to be: have, was, is,\n",
      "Nearest to d: b, repetitive, bpp,\n",
      "Nearest to known: used, fastened, lemmy,\n",
      "Nearest to see: annoyed, afternoons, adversarial,\n",
      "Nearest to after: governorates, with, trinomial,\n",
      "Nearest to four: five, eight, three,\n",
      "Nearest to can: may, akita, reduces,\n",
      "Nearest to of: in, for, dek,\n",
      "Nearest to seven: eight, six, nine,\n",
      "Nearest to during: in, from, at,\n",
      "Average loss at step  32000 :  6.15386500537\n",
      "Average loss at step  34000 :  5.89484549236\n",
      "Average loss at step  36000 :  5.70850143564\n",
      "Average loss at step  38000 :  5.6937345891\n",
      "Average loss at step  40000 :  5.52361577833\n",
      "Nearest to is: was, are, has,\n",
      "Nearest to state: awards, goo, trinomial,\n",
      "Nearest to also: abraxas, which, not,\n",
      "Nearest to no: sourceforge, antipsychotics, it,\n",
      "Nearest to world: vma, johansson, maneuverable,\n",
      "Nearest to system: dryness, bedouin, kashmir,\n",
      "Nearest to be: have, was, is,\n",
      "Nearest to d: b, repetitive, UNK,\n",
      "Nearest to known: used, fastened, lemmy,\n",
      "Nearest to see: annoyed, afternoons, and,\n",
      "Nearest to after: dago, governorates, with,\n",
      "Nearest to four: six, three, eight,\n",
      "Nearest to can: may, would, could,\n",
      "Nearest to of: in, for, and,\n",
      "Nearest to seven: eight, six, nine,\n",
      "Nearest to during: in, from, at,\n",
      "Average loss at step  42000 :  5.42518092203\n",
      "Average loss at step  44000 :  5.25600208724\n",
      "Average loss at step  46000 :  5.28727785313\n",
      "Average loss at step  48000 :  5.37750280499\n",
      "Average loss at step  50000 :  5.27576764035\n",
      "Nearest to is: was, are, has,\n",
      "Nearest to state: awards, cem, trinomial,\n",
      "Nearest to also: which, abraxas, it,\n",
      "Nearest to no: sourceforge, a, antipsychotics,\n",
      "Nearest to world: johansson, vma, kapoor,\n",
      "Nearest to system: dryness, wct, kapoor,\n",
      "Nearest to be: have, is, was,\n",
      "Nearest to d: b, repetitive, m,\n",
      "Nearest to known: used, fastened, lemmy,\n",
      "Nearest to see: but, astrobiology, annoyed,\n",
      "Nearest to after: wct, in, with,\n",
      "Nearest to four: six, five, eight,\n",
      "Nearest to can: may, would, will,\n",
      "Nearest to of: in, thibetanus, and,\n",
      "Nearest to seven: eight, six, five,\n",
      "Nearest to during: in, from, at,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "\n",
    "    init.run()\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0 and step > 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 3  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `Word2Vec` package to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/rouseguy/DeepLearning-NLP/blob/master/notebooks/2.%20word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_file = './data/juliuscaesar.txt'\n",
    "stopword_file  = './data/long_stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_wordsstop_wo  = []\n",
    "with open(stopword_file,'r') as inpFile:\n",
    "    lines = inpFile.readlines()\n",
    "    stop_words_temp = map(lambda x : re.sub('\\n','',x),lines)\n",
    "    stop_words = list(map(lambda x:  re.sub('[^A-Za-z0-9]+', '',x), stop_words_temp))\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    word = word.strip()\n",
    "    word = word.lower()\n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "    if word not in stop_words:\n",
    "        return word\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: they'll --> Cleaned: \n"
     ]
    }
   ],
   "source": [
    "print('Raw: ' + r\"they'll\" + ' --> Cleaned: ' + clean(\"they'll\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: King's --> Cleaned: kings\n"
     ]
    }
   ],
   "source": [
    "print('Raw: ' + r\"King's\" + ' --> Cleaned: ' + clean(\"King's\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentence from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = 0\n",
    "sentences = []\n",
    "          \n",
    "with open(caesar_file,'r') as inpFile:\n",
    "    x = inpFile.readlines()\n",
    "    for line in x:\n",
    "        if line is not None or line != '\\n':\n",
    "            words = line.split()\n",
    "            words = map(lambda x: clean(x), words)\n",
    "            words = list(filter(lambda x:True if len(x) > 0 else False, words))\n",
    "            sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['second', 'commoner', 'sir', 'wear', 'shoes'],\n",
       " [],\n",
       " ['work', 'sir', 'holiday']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[107:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = Word2Vec (sentences, \n",
    "                 window=5, \n",
    "                 size=500, \n",
    "                 workers=4, \n",
    "                 min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector length is: 500\n",
      "The vector for word \"second\" is : \n",
      "[ -1.04607592e-04   8.00557493e-04   7.22333963e-04  -5.17591388e-06\n",
      "   1.67458638e-04]\n"
     ]
    }
   ],
   "source": [
    "# Example output:\n",
    "print('The vector length is: ' + str(len(model.wv['second'])))\n",
    "print('The vector for word \"second\" is : ')\n",
    "print(model.wv['second'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brutus', 0.4171747863292694),\n",
       " ('casca', 0.3478987216949463),\n",
       " ('caesar', 0.3422013819217682),\n",
       " ('will', 0.33033287525177),\n",
       " ('thee', 0.3258987069129944),\n",
       " ('gods', 0.320721298456192),\n",
       " ('antony', 0.3149747848510742),\n",
       " ('cassius', 0.31106653809547424),\n",
       " ('good', 0.3107137680053711),\n",
       " ('lucius', 0.29914504289627075)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['rome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "381px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
