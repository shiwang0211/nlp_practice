{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Lambda, Activation, dot, concatenate, TimeDistributed\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "# https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "from script.seq2seq import generateInOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple character level Seq2Seq Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "- Bidirectional LSTM\n",
    "    * Allow information from future inputs\n",
    "    * LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN in Keras\n",
    "https://keras.io/layers/recurrent/\n",
    "\n",
    "    * keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "    * keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "    * 3D tensor with shape (batch_size, timesteps, input_dim).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data,\\\n",
    "decoder_input_data,\\\n",
    "decoder_target_data,\\\n",
    "num_samples, \\\n",
    "input_vocabulary, \\\n",
    "output_vocabulary, \\\n",
    "input_sequence_length, \\\n",
    "output_sequence_length, \\\n",
    "input_token_index, \\\n",
    "target_token_index = \\\n",
    "generateInOut(data_path = './data/fra.txt', num_samples = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16, 71)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape # num_sample, input_sequence_length, input_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 59, 94)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape # num_sample, output_seq_length, output_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 59, 94)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape # num_sample, output_seq_length, output_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\t', '\\n')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_target_token_index = {}\n",
    "for key, value in target_token_index.items():\n",
    "    re_target_token_index[value] = key\n",
    "re_target_token_index[0], re_target_token_index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Encoder: 1-direction LSTM\n",
    "- Decoder: 1-direction LSTM\n",
    "- Embedding: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "lstm_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if use word instead of character:\n",
    "https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Input\n",
    "encoder_inputs = Input(shape=(input_sequence_length, input_vocabulary)) # or (None, input_vocabulary)\n",
    "\n",
    "# Define Encoder Layer itself\n",
    "encoder = LSTM(lstm_units, return_state=True)\n",
    "\n",
    "# Define Encoder Output: Output, hidden state 'h' and 'c'\n",
    "e_all_h,e_last_h,e_last_c = encoder(encoder_inputs)\n",
    "\n",
    "# For Encoder, Output is not used\n",
    "encoder_states = [e_last_h, e_last_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network (V1) - Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Decoder Input\n",
    "decoder_inputs = Input(shape=(output_sequence_length, output_vocabulary))\n",
    "\n",
    "# Define Decoder Layer itself, note the difference with encoder\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# Extract Output, note the differene: \n",
    "# initial state is given from encoder, instead of default (Zero??)\n",
    "d_all_h,d_last_h,d_last_c = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(output_vocabulary, activation='softmax')\n",
    "decoder_outputs = decoder_dense(d_all_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network (V2) - Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, sequence length is \"ONE\"\n",
    "decoder_inputs = Input(shape=(1, output_vocabulary))\n",
    "\n",
    "# Same Decoder\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# Same Dense Layer\n",
    "decoder_dense = Dense(output_vocabulary, activation='softmax')\n",
    "\n",
    "# Define some lists\n",
    "final_outputs = []\n",
    "previous_states = encoder_states # States from encoder\n",
    "current_inputs = decoder_inputs # Start sentence index\n",
    "\n",
    "# Generate States, Outputs one-by-one\n",
    "for _ in range(output_sequence_length):\n",
    "    \n",
    "    d_all_h, d_last_h, d_last_c = decoder_lstm(current_inputs, initial_state = previous_states)\n",
    "    densed_output = decoder_dense(d_all_h) \n",
    "    final_outputs.append(densed_output)\n",
    "    \n",
    "    current_inputs = densed_output\n",
    "    previous_states = [d_last_h, d_last_c]\n",
    "\n",
    "# Concatenate all predictions\n",
    "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Sentence one-hot encoding\n",
    "target_token_index['\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERWRITE decoder_input_data to be \"START\" character\n",
    "decoder_input_data = np.zeros(shape = (num_samples, 1, output_vocabulary))\n",
    "decoder_input_data[:, 0, target_token_index['\\t']] = 1.\n",
    "decoder_input_data.shape  # num_sample, output_seq_length = 1, output_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, only apply to 2nd type of training method (using sampling)\n",
    "probs = model.predict([encoder_input_data[:5,:,:], decoder_input_data[:5,:,:]])\n",
    "predictions = np.argmax(probs, axis = 1)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism with Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use reverse index to generate actual sentence until first \"end of sentence\" character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- https://medium.com/datalogue/attention-in-keras-1892773a4f22\n",
    "- https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py\n",
    "- https://guillaumegenthial.github.io/sequence-to-sequence.html\n",
    "- https://github.com/wanasit/katakana/blob/master/notebooks/Attention-based%20Sequence-to-Sequence%20in%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate simple test case to show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(string.ascii_letters)\n",
    "num_samples = 10000\n",
    "input_vocabulary = 52 \n",
    "output_vocabulary = 52 + 2\n",
    "input_sequence_length = 10\n",
    "output_sequence_length = 10 + 1\n",
    "\n",
    "encoder_input_data = np.zeros(shape = (num_samples, input_sequence_length, input_vocabulary))\n",
    "decoder_input_data = np.zeros(shape = (num_samples, output_sequence_length, output_vocabulary))\n",
    "decoder_target_data = np.zeros(shape = (num_samples, output_sequence_length, output_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    a1 = []\n",
    "    b1 = []\n",
    "    c1 = []\n",
    "    \n",
    "    for j in range(input_sequence_length):\n",
    "        if len(c1) > 0:\n",
    "            b1.append(c1[-1])\n",
    "        else:\n",
    "            b1.append(52)\n",
    "        a1.append(np.random.randint(26))\n",
    "        c1.append(a1[-1] + 26)\n",
    "    b1.append(c1[-1])\n",
    "    c1.append(53)\n",
    "    \n",
    "    for j in range(input_sequence_length):\n",
    "        encoder_input_data[i, j, a1[j]] = 1\n",
    "        \n",
    "    for j in range(output_sequence_length):\n",
    "        decoder_input_data[i, j, b1[j]] = 1 \n",
    "        decoder_target_data[i, j, c1[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 17, 5, 13, 1, 3, 16, 13, 14, 21] \n",
      " [52, 37, 43, 31, 39, 27, 29, 42, 39, 40, 47] \n",
      " [37, 43, 31, 39, 27, 29, 42, 39, 40, 47, 53]\n"
     ]
    }
   ],
   "source": [
    "print(a1,'\\n', b1, '\\n',c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define encoder and teacher-forcing decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See section 1 for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 64\n",
    "lstm_units = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unroll: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lstm_9/transpose_1:0' shape=(?, 10, 50) dtype=float32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(input_sequence_length, input_vocabulary))\n",
    "encoder = LSTM(lstm_units, return_state=True, return_sequences = True,unroll=True) # Note, sequence\n",
    "e_all_h, e_last_h, e_last_c = encoder(encoder_inputs)\n",
    "encoder_states = [e_last_h, e_last_c]\n",
    "e_all_h # num_examples, input_sequence_length, lstm_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lstm_10/transpose_1:0' shape=(?, 11, 50) dtype=float32>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(output_sequence_length, output_vocabulary))\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True,unroll=True)\n",
    "d_all_h, d_last_h, d_last_c= decoder_lstm(decoder_inputs,\n",
    "                                     initial_state = encoder_states)\n",
    "d_all_h # num_examples, output_sequence_length, lstm_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Luong attention mechanism\n",
    "- Alignment function: dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dot_9/MatMul:0' shape=(?, 11, 10) dtype=float32>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = dot([d_all_h, e_all_h], axes=[2, 2])\n",
    "attention # num_examples, output_sequence_length, input_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf e_t = [s^T_{t-1} h_1, ..., s^T_{t-1} h_N] $<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attention_4/truediv:0' shape=(?, 11, 10) dtype=float32>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = Activation('softmax', name='attention')(attention) # Note Layer Name: \"attention\"\n",
    "attention # num_examples, output_sequence_length, input_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\pmb\\alpha_t = softmax(\\mathbf e_t) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dot_10/MatMul:0' shape=(?, 11, 50) dtype=float32>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = dot([attention, e_all_h], axes=[2,1])\n",
    "context #num_examples, output_sequence_length, lstm_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ c_t = \\sum_{k=1}^{N} \\alpha_{tk}h_k $<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_5/concat:0' shape=(?, 11, 100) dtype=float32>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_combined_context = concatenate([context, d_all_h])\n",
    "decoder_combined_context  # #num_examples, output_sequence_length, lstm_units * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ [c_t;h_t]$<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'time_distributed_10/Reshape_1:0' shape=(?, 11, 54) dtype=float32>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(output_vocabulary, activation=\"softmax\"))(output)\n",
    "output #num_examples, output_sequence_length, output_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\tilde h_t = tanh(W_c[c_t;h_t])$<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiwang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_9/mul_59:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'lstm_9/add_77:0' shape=(?, 50) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('./model/seq2seq.h5')\n",
    "# model = load_model('./model/seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x1a37f0f668>,\n",
       " <keras.engine.topology.InputLayer at 0x1a3301bc18>,\n",
       " <keras.layers.recurrent.LSTM at 0x1a37f38358>,\n",
       " <keras.layers.recurrent.LSTM at 0x1a3301b828>,\n",
       " <keras.layers.merge.Dot at 0x1a334084e0>,\n",
       " <keras.layers.core.Activation at 0x1a334083c8>,\n",
       " <keras.layers.merge.Dot at 0x1a3341f470>,\n",
       " <keras.layers.merge.Concatenate at 0x1a3301bba8>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x1a3341f630>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x1a3341fac8>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild model to have intermediate outputs:\n",
    "- Original model outputs: `model.outputs`\n",
    "- Attention layer outputs: `attention_layer.output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 11, 54), (None, 11, 10)]\n"
     ]
    }
   ],
   "source": [
    "attention_layer = model.get_layer('attention') # Extract Layer by layer name\n",
    "attention_model = Model(inputs = model.inputs, \n",
    "                        outputs = model.outputs + [attention_layer.output])\n",
    "print(attention_model.output_shape)\n",
    "# num_examples, output_sequence_length, output_vocabulary\n",
    "# num_examples, output_sequence_length, input_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Simple test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = Input(shape=(input_sequence_length, input_vocabulary))\n",
    "# decoder_inputs = Input(shape=(output_sequence_length, output_vocabulary))\n",
    "# target_token_index['\\t'], target_token_index['\\n']) == (0,1)\n",
    "\n",
    "# Assume single sample\n",
    "num_sample = 1\n",
    "test_text = a1\n",
    "test_input = test_text\n",
    "\n",
    "# Encoder input\n",
    "encoder_input = np.zeros(shape = (num_sample, input_sequence_length, input_vocabulary), dtype = np.float32)\n",
    "for index, pos in enumerate(test_input):\n",
    "    encoder_input[0,index,pos] = 1\n",
    "\n",
    "# Decoder Input\n",
    "decoder_input = np.zeros(shape = (num_sample, output_sequence_length, output_vocabulary), dtype = np.float32)\n",
    "\n",
    "# Initialize first position to be SoS\n",
    "decoder_input[:,0,52] = 1 # Each sample, start of sentence, should be '\\t' (index 0: SoS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction by greedy sampling, step by step\n",
    "predictions = []\n",
    "\n",
    "for t in range(0, output_sequence_length):\n",
    "    \n",
    "    output, attention = attention_model.predict([encoder_input, decoder_input])\n",
    "    # output: num_examples, output_sequence_length, output_vocabulary\n",
    "    # attention: num_examples, output_sequence_length, input_sequence_length\n",
    "    \n",
    "    prediction_t_argmax = output.argmax(axis = 2)[:, t] # only pick lastest prdiction at time t\n",
    "    predictions.append(prediction_t_argmax[0])\n",
    "    # Stop with EoS\n",
    "    if prediction_t_argmax == 53:\n",
    "        break\n",
    "    decoder_input[:, t+1, prediction_t_argmax[0]] = 1 # overwrite decoder_input with latest t\n",
    "\n",
    "# Post-process for 1st sample\n",
    "attention_density = attention[0] # attention for 1st sample after last time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 5, 13, 1, 3, 16, 13, 14, 21]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 43, 31, 39, 27, 29, 42, 39, 40, 47, 53]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird results, to be investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nScgREQ3x/wEmJp4SnCNVzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a437835c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(attention_density,   \n",
    "        xticklabels=[w for w in test_input],\n",
    "        yticklabels=[w for w in predictions])\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://guillaumegenthial.github.io/sequence-to-sequence.html\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
