{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Seq2Seq Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "- Bidirectional LSTM\n",
    "    * Allow information from future inputs\n",
    "    * LSTM only allows past information\n",
    "<img src=\"https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=\"500\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN in Keras\n",
    "https://keras.io/layers/recurrent/\n",
    "\n",
    "    * keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "    * keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "    * 3D tensor with shape (batch_size, timesteps, input_dim).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.htmlReference:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Lambda\n",
    "from keras import backend as K\n",
    "# https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "from script.seq2seq import generateInOut\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data,\\\n",
    "decoder_input_data,\\\n",
    "decoder_target_data,\\\n",
    "num_samples, \\\n",
    "input_vocabulary, \\\n",
    "output_vocabulary, \\\n",
    "input_sequence_length, \\\n",
    "output_sequence_length, \\\n",
    "input_token_index, \\\n",
    "target_token_index = \\\n",
    "generateInOut(data_path = './data/fra.txt', num_samples = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16, 71)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape # num_sample, input_sequence_length, input_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 59, 94)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape # num_sample, output_seq_length, output_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 59, 94)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape # num_sample, output_seq_length, output_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Encoder: 1-direction LSTM\n",
    "- Decoder: 1-direction LSTM\n",
    "- Embedding: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 64\n",
    "lstm_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if use word instead of character:\n",
    "https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder Input\n",
    "encoder_inputs = Input(shape=(input_sequence_length, input_vocabulary)) # or (None, input_vocabulary)\n",
    "\n",
    "# Define Encoder itself\n",
    "encoder = LSTM(lstm_units, return_state=True)\n",
    "\n",
    "# Define Encoder Output: Output, hidden state 'h' and 'c'\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# For Encoder, Output is not used\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network (V1) - Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Decoder Input\n",
    "decoder_inputs = Input(shape=(output_sequence_length, output_vocabulary))\n",
    "\n",
    "# Define Decoder itself, note the difference with encoder\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# Extract Output, note the differene: \n",
    "# initial state is given from encoder, instead of default (Zero??)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(output_vocabulary, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network (V2) - Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, sequence length is \"ONE\"\n",
    "decoder_inputs = Input(shape=(1, output_vocabulary))\n",
    "\n",
    "# Same Decoder\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# Same Dense Layer\n",
    "decoder_dense = Dense(output_vocabulary, activation='softmax')\n",
    "\n",
    "# Define some lists\n",
    "final_outputs = []\n",
    "previous_states = encoder_states # States from encoder\n",
    "current_inputs = decoder_inputs # Start sentence index\n",
    "\n",
    "# Generate States, Outputs one-by-one\n",
    "for _ in range(output_sequence_length):\n",
    "    \n",
    "    outputs, state_h, state_c = decoder_lstm(current_inputs, initial_state = previous_states)\n",
    "    densed_output = decoder_dense(outputs) \n",
    "    final_outputs.append(densed_output)\n",
    "    \n",
    "    current_inputs = densed_output\n",
    "    previous_states = [state_h, state_c]\n",
    "\n",
    "# Concatenate all predictions\n",
    "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start of Sentence one-hot encoding\n",
    "target_token_index['\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 94)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OVERWRITE decoder_input_data to be \"START\" character\n",
    "decoder_input_data = np.zeros(shape = (num_samples, 1, output_vocabulary))\n",
    "decoder_input_data[:, 0, target_token_index['\\t']] = 1.\n",
    "decoder_input_data.shape  # num_sample, output_seq_length -> 1, output_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.9537 - val_loss: 1.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1863443d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 53, 16, 16,  0,  0,  1, 10,  0,  0,  0, 10, 58,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, 41,  4,  4,  4,  4,  4,  4,  2,  2,  4,  3,  0,  3,\n",
       "        4,  4,  4,  4,  4,  5,  4,  6,  3,  5,  3, 14, 11, 12, 13, 16, 10,\n",
       "       10, 11, 19,  0,  0, 14, 13, 16, 14, 12, 10, 16, 16, 16, 15, 11,  0,\n",
       "       10, 10, 10,  0,  0,  0,  2,  4,  2,  3, 10,  2,  0,  0, 13,  9,  0,\n",
       "        0,  0,  1,  0,  0,  0,  1,  0, 11])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note, only apply to 2nd type of training method (using sampling)\n",
    "probs = model.predict([encoder_input_data[:5,:,:], decoder_input_data[:5,:,:]])\n",
    "predictions = np.argmax(probs, axis = 1)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use reverse index to generate actual sentence until first \"end of sentence\" character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- https://medium.com/datalogue/attention-in-keras-1892773a4f22\n",
    "- https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py\n",
    "- https://guillaumegenthial.github.io/sequence-to-sequence.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://guillaumegenthial.github.io/sequence-to-sequence.html\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
